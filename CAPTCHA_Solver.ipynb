{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yatheesh73999/REFINE-CAPTCHA-/blob/main/CAPTCHA_Solver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcUPCkTny1Qw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "from torch.utils import data\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKID5n7myyhZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6qHFoDy6Vda"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxgnY-y1vMp4"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "from random import randint, choice, uniform\n",
        "import string\n",
        "import time\n",
        "\n",
        "fonts_drive_path = '/content/gdrive/My Drive/Miniproj/Fonts'\n",
        "colors = {\n",
        "  \"black\": 0x1c1c1c,\n",
        "  \"white\": 0xfcfcfc,\n",
        "}\n",
        "\n",
        "image_width = 250\n",
        "image_height = 70\n",
        "font_size = 50\n",
        "characters = [5,5]\n",
        "\n",
        "def get_text():\n",
        "  out_string = \"\"\n",
        "  for i in range(randint(characters[0], characters[1])):\n",
        "    out_string += choice(string.ascii_letters+\"0123456789\")\n",
        "  return out_string\n",
        "\n",
        "def draw_pixel(draw,x,y, thickness):\n",
        "  if(thickness > 1):\n",
        "    draw.line([(x, y), (x+thickness*([1,-1][randint(0, 1)]),y+thickness*([1,-1][randint(0, 1)]))], fill=colors[\"black\"], width=thickness)\n",
        "  else:\n",
        "    draw.line([(x, y), (x,y)], fill=colors[\"black\"])\n",
        "\n",
        "def add_noise(draw, amount, thickness):\n",
        "  for i in range(int(amount)):\n",
        "    draw_pixel(draw, randint(0, image_width), randint(0, image_height), thickness)\n",
        "\n",
        "def add_lines(draw, amount, thickness):\n",
        "\n",
        "  thickness = 1\n",
        "  wiggle_room_thickness = 2\n",
        "\n",
        "  for i in range(int(amount)):\n",
        "    wiggle_thickness = randint(thickness,thickness+wiggle_room_thickness)\n",
        "    draw.line([(randint(0, image_width), randint(0, image_height)), (randint(0, image_width),randint(0, image_height))], fill=colors[\"black\"],width=wiggle_thickness,)\n",
        "\n",
        "def draw_characters(draw, image, text):\n",
        "\n",
        "  fonts = [\"ComicSansMS3.ttf\",\"carbontype.ttf\",\"Kingthings_Trypewriter_2.ttf\",\"Sears_Tower.ttf\",\"TravelingTypewriter.ttf\"]\n",
        "  wiggle_room_width = 48 - len(text) * 6\n",
        "  wiggle_room_height = 13\n",
        "  width_padding = 30\n",
        "  font_size = 30\n",
        "  wiggle_room_font_size = 10\n",
        "  rotation_degrees = 20\n",
        "  wiggle_room_rotation_percent = 0.4\n",
        "\n",
        "  spacing_width = (image_width-width_padding) / len(text)\n",
        "  next_letter_pos = width_padding/2\n",
        "\n",
        "\n",
        "  for character in text:\n",
        "    wiggle_width = uniform(0,wiggle_room_width)\n",
        "    wiggle_height = uniform(0,wiggle_room_height)\n",
        "    wiggle_font_size = uniform(font_size - wiggle_room_font_size / 2, font_size  + wiggle_room_font_size / 2)\n",
        "    wiggle_rotation_degrees = rotation_degrees * uniform( -wiggle_room_rotation_percent,  wiggle_room_rotation_percent)\n",
        "    font = ImageFont.truetype(fonts_drive_path+\"/\"+fonts[randint(0,len(fonts)-1)], int(wiggle_font_size))\n",
        "    character_image = Image.new(\"RGB\", (image_height, image_height), 255)\n",
        "    draw = ImageDraw.Draw(character_image)\n",
        "    draw.rectangle([0, 0, image_height, image_height], fill=colors[\"white\"])\n",
        "    draw.text((wiggle_width, wiggle_height), character, fill=colors[\"black\"], font=font)\n",
        "    character_image = character_image.rotate(wiggle_rotation_degrees, expand=True, fillcolor=\"white\")\n",
        "    image.paste(character_image, (int(next_letter_pos), 0))\n",
        "    next_letter_pos += spacing_width\n",
        "\n",
        "def generate_data(num_samples, drive_path):\n",
        "  text_done = []\n",
        "  start = time.time()\n",
        "  for i in range(int(num_samples)):\n",
        "\n",
        "    image = Image.new('RGB',(image_width, image_height))\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    draw.rectangle([0, 0, image_width, image_height], fill=colors[\"white\"])\n",
        "\n",
        "    text = get_text()\n",
        "    if(text not in text_done):\n",
        "      text_done.append(text)\n",
        "    else:\n",
        "      print(\"Skipped\")\n",
        "      continue\n",
        "    draw_characters(draw, image, text)\n",
        "\n",
        "    add_noise(draw, amount=int(image_width*image_height*0.001), thickness = 1)\n",
        "    add_noise(draw, amount=int(image_width*image_height*0.001), thickness = 2)\n",
        "    add_noise(draw, amount=int(image_width*image_height*0.001), thickness = 3)\n",
        "    add_lines(draw, amount=1, thickness=2)\n",
        "    add_lines(draw, amount=1, thickness=3)\n",
        "\n",
        "\n",
        "    image.save(drive_path+\"/\"+text+\".jpg\")\n",
        "  end = time.time()\n",
        "  print(\"Operation completed. Time taken: \"+str(end - start)+\" seconds\")\n",
        "\n",
        "num_samples_train = input(\"How many images do you want to generate for training?\")\n",
        "drive_path_train = '/content/gdrive/My Drive/Miniproj/DS/Saved_Train'\n",
        "num_samples_validation = input(\"How many images do you want to generate for validation?\")\n",
        "drive_path_validation = '/content/gdrive/My Drive/Miniproj/DS/Saved_Validation'\n",
        "num_samples_testing = input(\"How many images do you want to generate for testing?\")\n",
        "drive_path_testing = '/content/gdrive/My Drive/Miniproj/DS/Saved_Test'\n",
        "generate_data(num_samples_train, drive_path_train)\n",
        "generate_data(num_samples_validation, drive_path_validation)\n",
        "generate_data(num_samples_testing, drive_path_testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2l_tlvZ72WI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hpnipDU6cdh"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwyRgc7jMe6k"
      },
      "outputs": [],
      "source": [
        "#Enabling GPU Usage\n",
        "use_cuda = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcWjNvWaBlKx"
      },
      "outputs": [],
      "source": [
        "all_characters = string.ascii_letters+\"0123456789\"\n",
        "all_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnUhEuBSB9yv"
      },
      "outputs": [],
      "source": [
        "def string_label(string):\n",
        "  out = []\n",
        "  index = 0\n",
        "  for char_string in string:\n",
        "    for char_index in range(0,len(all_characters)):\n",
        "      if char_string == all_characters[char_index]:\n",
        "        out.append(int(char_index))\n",
        "        break\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGRceeM9_I7v"
      },
      "outputs": [],
      "source": [
        "class CAPTCHAImageDataset(data.Dataset):\n",
        "  def __init__(self, img_dir, transform=None):\n",
        "    self.all_images = os.listdir(img_dir)\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.all_images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.all_images[idx]\n",
        "    # print(img_path)\n",
        "    label = img_path.split(\"/\")[-1]\n",
        "    label = img_path.split(\"-\")[0]\n",
        "    label = label.replace(\".jpg\",\"\")\n",
        "    # print(label)\n",
        "    image = Image.open(self.img_dir + \"/\" + img_path)\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    labels = torch.Tensor(string_label(label))\n",
        "    labels = labels.type(torch.LongTensor)\n",
        "    labels = labels.squeeze(0)\n",
        "    return image, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99MGbr-_6gc3"
      },
      "source": [
        "# Data Pre Processing\n",
        "Process the data by adding a filter, inverting colors, ensuring black and white"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vmJlZYqCgBI"
      },
      "outputs": [],
      "source": [
        "# Image pre-processing\n",
        "def remove_noise_and_grayscale(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  gray_median = cv2.medianBlur(gray_img, 3)\n",
        "  gray_thresh = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY_INV, cv2.THRESH_OTSU)[1]\n",
        "  return gray_thresh\n",
        "\n",
        "# cv2_imshow(cv2.imread('/content/gdrive/My Drive/Colab Notebooks/Datasets/project_dataset/Training/0By1b.jpg'))\n",
        "# gray_img = remove_noise_and_grayscale('/content/gdrive/My Drive/Colab Notebooks/Datasets/project_dataset/Training/0By1b.jpg')\n",
        "# cv2_imshow(gray_img)\n",
        "\n",
        "def generate_processed_data_folder(input_path, output_path):\n",
        "  # shutil.rmtree(output_path)\n",
        "  if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "    print(output_path)\n",
        "  for dir in os.listdir(input_path):\n",
        "    out_dir_path = os.path.join(output_path, dir)\n",
        "    if not os.path.exists(out_dir_path):\n",
        "      os.mkdir(out_dir_path)\n",
        "      print(out_dir_path)\n",
        "    in_dir_path = os.path.join(input_path, dir)\n",
        "    print(\"Processing images in: \" + dir)\n",
        "    for img in os.listdir(in_dir_path):\n",
        "      if os.path.isfile(os.path.join(in_dir_path, img)) and not os.path.isfile(os.path.join(out_dir_path, img)):\n",
        "        in_img_path = os.path.join(in_dir_path, img)\n",
        "        out_img_path = os.path.join(out_dir_path, img)\n",
        "        processed_img = remove_noise_and_grayscale(in_img_path)\n",
        "        print(in_img_path, out_img_path)\n",
        "        if not cv2.imwrite(out_img_path, processed_img):\n",
        "          raise Exception(\"Could not write image: {}\".format(out_img_path))\n",
        "  print(\"Done processing\")\n",
        "\n",
        "# def create_baseline_split_data(input_path, output_path):\n",
        "#   if not os.path.exists(output_path):\n",
        "#     os.mkdir(output_path)\n",
        "\n",
        "#   for dir in os.listdir(input_path):\n",
        "#     out_dir_path = os.path.join(output_path, dir)\n",
        "#     if not os.path.exists(output_path):\n",
        "#       os.mkdir(out_dir_path)\n",
        "#     in_dir_path = os.path.join(input_path, dir)\n",
        "#     print(\"Processing images in: \" + dir)\n",
        "#     image_count = 0\n",
        "#     for img in os.listdir(in_dir_path):\n",
        "#       image_count+=1\n",
        "#       if os.path.isfile(os.path.join(in_dir_path, img)):\n",
        "#         in_img_path = os.path.join(in_dir_path, img)\n",
        "#         transform = transforms.Compose([transforms.PILToTensor()])\n",
        "#         label = in_img_path.split(\"/\")[-1]\n",
        "#         label = label.replace(\".jpg\",\"\")\n",
        "#         current_label = 0\n",
        "#         img_pil = Image.open(in_img_path)\n",
        "#         num_chars = 5\n",
        "#         for index in range(0,num_chars):\n",
        "#           w,h = img_pil.size\n",
        "#           start_w = int(index * w/num_chars)\n",
        "#           end_w = int(min(w, (index+1) * w/num_chars))\n",
        "#           img_split = img_pil.crop((start_w,0,end_w,h))\n",
        "#           img_split.show()\n",
        "#           if not os.path.exists(out_dir_path+\"/\"+str(label[current_label])):\n",
        "#             os.mkdir(out_dir_path+\"/\"+str(label[current_label]))\n",
        "#           out_img_path = os.path.join(out_dir_path+\"/\"+str(label[current_label]), str(label[current_label])+\"-\"+str(image_count)+\".jpg\")\n",
        "#           img_split.save(out_img_path)\n",
        "#           current_label+=1\n",
        "#           image_count+=1\n",
        "#     print(\"Processed:\", image_count, \"character images\")\n",
        "#   print(\"Done processing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHB9UIzj0oF1"
      },
      "outputs": [],
      "source": [
        "def get_data_loader(batch_size, is_split_data=False):\n",
        "  transform = transforms.Compose([transforms.Resize((70,250)),transforms.ToTensor()])\n",
        "  if is_split_data==True:\n",
        "    transform = transforms.Compose([transforms.Resize((224,1120)),transforms.ToTensor()])\n",
        "  train_data = CAPTCHAImageDataset(drive_path + \"/Training\", transform=transform)\n",
        "  val_data = CAPTCHAImageDataset(drive_path + \"/Validation\", transform=transform)\n",
        "  test_data = CAPTCHAImageDataset(drive_path + \"/Testing\", transform=transform)\n",
        "\n",
        "  # print(train_data.__getitem__(1))\n",
        "  train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,num_workers=1,shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,num_workers=1,shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,num_workers=1,shuffle=True)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EXwEklI0rbi"
      },
      "outputs": [],
      "source": [
        "# Process data for the baseline model (estimate where the character is and split up the image accordingly)\n",
        "# Data splitting now happens when training the baseline model!\n",
        "# drive_path = '/content/gdrive/My Drive/Colab Notebooks/Datasets'\n",
        "# create_baseline_split_data(drive_path + '/project_dataset_processed', drive_path + '/baseline_split')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJyblq4y0sbM"
      },
      "outputs": [],
      "source": [
        "# drive_path = '/content/gdrive/My Drive/Colab Notebooks/Datasets/baseline_split'\n",
        "# train_loader, val_loader, test_loader = get_data_loader(1, is_split_data=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znk_-akhMwJf"
      },
      "outputs": [],
      "source": [
        "os.makedirs('/content/drive/MyDrive/Miniproj', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxzrK-xuNPNt"
      },
      "outputs": [],
      "source": [
        "# Process the data by adding a filter, inverting colors, ensuring black and white\n",
        "drive_path = '/content/drive/MyDrive/Miniproj'\n",
        "#Corrected the output path to remove the extra drive path\n",
        "generate_processed_data_folder(os.path.join(drive_path, 'DS'), os.path.join(drive_path, 'DS/Processed'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iqYGzZjxJKi"
      },
      "outputs": [],
      "source": [
        "drive_path = '/content/drive/MyDrive/Miniproj/DS'\n",
        "project_dataset_path = drive_path + '/content/drive/MyDrive/Miniproj/DS'\n",
        "project_dataset_p_path = drive_path + '/content/drive/MyDrive/Miniproj/DS/Processed'\n",
        "print(len(os.listdir(os.path.join(project_dataset_path, '/content/drive/MyDrive/Miniproj/DS/Saved_Validation'))))\n",
        "print(len(os.listdir(os.path.join(project_dataset_p_path, '/content/drive/MyDrive/Miniproj/DS/Saved_Validation'))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIV4wcg10mZX"
      },
      "outputs": [],
      "source": [
        "drive_path = '/content/drive/MyDrive/Miniproj/DS/Processed'\n",
        "train_loader, val_loader, test_loader = get_data_loader(168)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piX_iZQ46UOH"
      },
      "outputs": [],
      "source": [
        "len(train_loader)\n",
        "#for images, labels in train_loader:\n",
        " # print(labels)\n",
        "  #print(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOgDfoyY6UyF"
      },
      "outputs": [],
      "source": [
        "len(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoeTopaVznG-"
      },
      "outputs": [],
      "source": [
        "len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POGH2DTj6V_Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_image(image):\n",
        "    image = torch.squeeze(image, dim=0)  # Remove batch dimension\n",
        "    img_np = image.numpy()\n",
        "\n",
        "    # Fix dimension issue before plotting\n",
        "    if img_np.shape[0] == 3:  # RGB Image (C, H, W)\n",
        "        plt.imshow(img_np.transpose(1, 2, 0))  # Convert to (H, W, C)\n",
        "    elif img_np.shape[0] == 1:  # Grayscale Image (1, H, W)\n",
        "        plt.imshow(img_np.squeeze(0), cmap='gray')  # Remove channel dimension\n",
        "    else:\n",
        "        plt.imshow(img_np)  # Already in (H, W, C) format\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "# Assuming `image` is a tensor with shape (C, H, W)\n",
        "image = torch.randn(1, 28, 28)  # Example grayscale image\n",
        "show_image(image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOvvjOXD6q1o"
      },
      "source": [
        "# Baseline CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1qvaC5kMaEA"
      },
      "outputs": [],
      "source": [
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "  \"\"\" Generate a name for the model consisting of all the hyperparameter values\"\"\"\n",
        "  path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,batch_size,learning_rate,epoch)\n",
        "  return path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycyxID93Mak1"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, data_loader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for imgs, labels in data_loader:\n",
        "    all_imgs_array = []\n",
        "    all_imgs = torch.Tensor()\n",
        "    for img in imgs:\n",
        "      split_imgs = split_image(img)\n",
        "      for split_img in split_imgs:\n",
        "        all_imgs_array.append(split_img)\n",
        "    all_imgs = torch.stack(all_imgs_array)\n",
        "\n",
        "    all_labels = torch.flatten(labels, start_dim=0, end_dim=1)\n",
        "\n",
        "    #To Enable GPU Usage\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "      all_imgs = all_imgs.cuda()\n",
        "      all_labels = all_labels.cuda()\n",
        "\n",
        "    output = model(all_imgs)\n",
        "    #select index with maximum prediction score\n",
        "    pred = output.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(all_labels.view_as(pred)).sum().item()\n",
        "    total += all_imgs.shape[0]\n",
        "    if total == 0:\n",
        "      return 0\n",
        "  return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSJq8bGxH0Fn"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cpu')\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88coIwsvMUo0"
      },
      "outputs": [],
      "source": [
        "class CNNCharNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNNCharNet, self).__init__()\n",
        "    self.name = \"CNNCharNet\"\n",
        "    # (224 - 5 + 2*0) / 1 + 1 = 220 x 220 (5 channels)\n",
        "    self.conv1 = nn.Conv2d(1,5,5)\n",
        "    # (220 - 2 + 2*0) / 2 + 1 = 110 x 110 (5 channels)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    # (110 - 5 + 2*0) / 1 + 1 = 106 x 106 (10 channels)\n",
        "    self.conv2 = nn.Conv2d(5,10,5)\n",
        "    # (106 - 2 + 2*0) / 2 + 1 = 53 x 53 (10 channels)\n",
        "    # self.fc1 = nn.Linear(53*53*10, 32)\n",
        "    # self.fc2 = nn.Linear(32,len(all_characters))\n",
        "    self.fc1 = nn.Linear(53*53*10, len(all_characters))\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1,53*53*10)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.softmax(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gYzgRO-eqgB"
      },
      "outputs": [],
      "source": [
        "def split_image(img, num_chars=5):\n",
        "  imgs = []\n",
        "  c, h, w = img.size()\n",
        "  for i in range(num_chars):\n",
        "    start_w = int(i * w/num_chars)\n",
        "    end_w = int(min(w, (i+1) * w/num_chars))\n",
        "    img_cropped = img[:, :, start_w:end_w]\n",
        "    imgs.append(img_cropped)\n",
        "    # print(img_cropped.size())\n",
        "  return imgs\n",
        "\n",
        "def train(model, train_loader, val_loader, test_laoder, batch_size=64, learning_rate=0.001, num_epochs=30):\n",
        "  # Check GPU availability\n",
        "  if use_cuda and torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "  else:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "\n",
        "  # Fixed PyTorch random seed for reproducible result\n",
        "  torch.manual_seed(1000)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  iters, losses, train_acc, val_acc = [], [], [], []\n",
        "\n",
        "  print(\"Training...\")\n",
        "  start_time = time.time()\n",
        "\n",
        "  # training\n",
        "  n = 0 # the number of iterations\n",
        "  # Set up some numpy arrays to store the accuracy\n",
        "  for epoch in range(num_epochs):\n",
        "    for imgs, labels in iter(train_loader):\n",
        "      # print(labels)\n",
        "      all_imgs_array = []\n",
        "      all_imgs = torch.Tensor()\n",
        "      # print(imgs.size())\n",
        "      for img in imgs:\n",
        "        split_imgs = split_image(img)\n",
        "        for split_img in split_imgs:\n",
        "          all_imgs_array.append(split_img)\n",
        "      all_imgs = torch.stack(all_imgs_array)\n",
        "\n",
        "      all_labels = torch.flatten(labels, start_dim=0, end_dim=1)\n",
        "\n",
        "      #To Enable GPU Usage\n",
        "      if use_cuda and torch.cuda.is_available():\n",
        "        all_imgs = all_imgs.cuda()\n",
        "        all_labels = all_labels.cuda()\n",
        "      out = model(all_imgs)         # forward pass\n",
        "      loss = criterion(out, all_labels) # compute the total loss\n",
        "      loss.backward()               # backward pass (compute parameter updates)\n",
        "      optimizer.step()              # make the updates for each parameter\n",
        "      optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "    # save the current training information\n",
        "    iters.append(n)\n",
        "    losses.append(float(loss)/batch_size)             # compute *average* loss\n",
        "    train_acc.append(get_accuracy(model, train_loader)) # compute training accuracy\n",
        "    val_acc.append(get_accuracy(model, val_loader))  # compute validation accuracy\n",
        "    n += 1\n",
        "\n",
        "    print((\"Epoch {}: Train accuracy: {} | Validation accuracy: {}\").format(epoch + 1,train_acc[epoch],val_acc[epoch]))\n",
        "\n",
        "    # Save the current model (checkpoint) to a file\n",
        "    model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "  print('Finished Training')\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "\n",
        "  # plotting\n",
        "  import matplotlib.pyplot as plt\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters, losses, label=\"Train\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters, train_acc, label=\"Train\")\n",
        "  plt.plot(iters, val_acc, label=\"Validation\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Training Accuracy\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "  print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC6DJW13z0KP"
      },
      "outputs": [],
      "source": [
        "# Process data for the baseline model (estimate where the character is and split up the image accordingly)\n",
        "drive_path = '/content/drive/MyDrive/Miniproj/DS/Processed'\n",
        "train_loader, val_loader, test_loader = get_data_loader(64, is_split_data=True)\n",
        "model = CNNCharNet()\n",
        "train(model, train_loader, val_loader, test_loader, batch_size=64, learning_rate=0.001, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVdqah5vR6Hv"
      },
      "outputs": [],
      "source": [
        "# Get the accuracy of the baseline\n",
        "model_path = get_model_name(model.name, batch_size=64, learning_rate=0.001, epoch=20-1)\n",
        "print(model_path)\n",
        "state = torch.load(model_path)\n",
        "model.load_state_dict(state)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "get_accuracy(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSD_9LvL6vnB"
      },
      "source": [
        "# Primary Model\n",
        "Bidrectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw6l17sPYx3b"
      },
      "outputs": [],
      "source": [
        "for img, labels in test_loader:\n",
        "  print(img)\n",
        "  print(labels)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjbAOsNqqS64"
      },
      "outputs": [],
      "source": [
        "# # Bidirectional recurrent neural network\n",
        "# class BiRNN(nn.Module):\n",
        "#     def __init__(self, input_size, n_hidden, n_layers):\n",
        "#         super(BiRNN, self).__init__()\n",
        "#         self.name = 'BiRNN'\n",
        "#         self.input_size = input_size\n",
        "#         self.n_hidden = n_hidden\n",
        "#         self.n_layers = n_layers\n",
        "#         self.lstm = nn.LSTM(input_size, n_hidden, n_layers, batch_first=True, bidirectional=True)\n",
        "#         self.fc = nn.Linear(n_hidden*2, 1) #2 for bidirectional\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Set initial states\n",
        "#         h_0 = torch.zeros(self.n_layers*2, x.size(0), self.n_hidden)\n",
        "#         c_0 = torch.zeros(self.n_layers*2, x.size(0), self.n_hidden)\n",
        "\n",
        "#         # Forward propagate LSTM\n",
        "#         output, hidden = self.lstm(x, (h_0, c_0))\n",
        "\n",
        "#         # Decode the hidden state of the last time step\n",
        "#         output = self.fc(output[:, -1, :])\n",
        "#         output = F.softmax(output)\n",
        "#         return output\n",
        "\n",
        "# model = BiRNN(input_size = len(all_characters),n_hidden = 10,n_layers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDiswu7YFZMX"
      },
      "outputs": [],
      "source": [
        "# class CAPTCHANet(nn.Module):\n",
        "#   def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "#     super(CAPTCHANet, self).__init__()\n",
        "#     self.name = \"CAPTCHANet\"\n",
        "#     self.emb = torch.eye(input_size)\n",
        "#     self.hidden_size = hidden_size\n",
        "#     self.num_layers = num_layers\n",
        "#     self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional = True)\n",
        "#     self.fc = nn.Linear(hidden_size * 2, num_classes) # multiply by 2 -> one forward, one backward\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # multiply by 2 -> one forward, one backward\n",
        "#     c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # multiply by 2 -> one forward, one backward\n",
        "\n",
        "#     print(x[0][0].size(0))\n",
        "#     print(len(all_characters))\n",
        "#     x = self.emb(x)\n",
        "#     print(\"OK\")\n",
        "#     out, _ = self.rnn(x, (h0, c0))\n",
        "#     out = torch.cat([torch.max(out, dim=1)[0], torch.mean(out, dim=1)], dim=1)\n",
        "#     return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0lu6FsAQxKA"
      },
      "outputs": [],
      "source": [
        "# model = CAPTCHANet(len(all_characters), hidden_size = 10, num_layers=2, num_classes = 1)\n",
        "# train_loader, val_loader, test_loader = get_data_loader(1)\n",
        "# train(model, train_loader, val_loader, num_epochs=5, learning_rate=0.001, batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoMv5ygNG-7_"
      },
      "outputs": [],
      "source": [
        "# def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-3, batch_size=256):\n",
        "#   # Check GPU availability\n",
        "#   if use_cuda and torch.cuda.is_available():\n",
        "#     model.cuda()\n",
        "#     print('CUDA is available!  Training on GPU ...')\n",
        "#   else:\n",
        "#     print('CUDA is not available.  Training on CPU ...')\n",
        "\n",
        "#   start_time = time.time()\n",
        "\n",
        "#   torch.manual_seed(42)\n",
        "#   criterion = nn.CrossEntropyLoss()\n",
        "#   optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#   train_acc, valid_acc, losses, iters = [], [], [], []\n",
        "#   n = 0 # the number of iterations\n",
        "\n",
        "#   for epoch in range(num_epochs):\n",
        "#     total_train_loss = 0.0\n",
        "#     for imgs, labels in iter(train_loader):\n",
        "#       if use_cuda and torch.cuda.is_available():\n",
        "#         imgs = imgs.cuda()\n",
        "#         labels = labels.cuda()\n",
        "#       pred = model(imgs)            # forward pass\n",
        "#       loss = criterion(pred, labels)# compute the total loss\n",
        "#       loss.backward()               # backward pass (compute parameter updates)\n",
        "#       optimizer.step()              # make the updates for each parameter\n",
        "#       optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "#     # save the current training information\n",
        "#     iters.append(n)\n",
        "#     losses.append(float(loss)/batch_size)               # compute *average* loss\n",
        "#     train_acc.append(get_accuracy(model, train_loader)) # compute training accuracy\n",
        "#     valid_acc.append(get_accuracy(model, val_loader))   # compute validation accuracy\n",
        "#     n += 1\n",
        "\n",
        "\n",
        "#     print((\"Epoch {}: Train accuracy: {} | Validation accuracy: {}\").format(epoch + 1,train_acc[epoch],val_acc[epoch]))\n",
        "\n",
        "#     # Save the current model (checkpoint) to a file\n",
        "#     model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n",
        "#     torch.save(model.state_dict(), model_path)\n",
        "\n",
        "#     print('Finished Training')\n",
        "#     end_time = time.time()\n",
        "#     elapsed_time = end_time - start_time\n",
        "#     print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "\n",
        "#     # plotting\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     plt.title(\"Training Curve\")\n",
        "#     plt.plot(iters, losses, label=\"Train\")\n",
        "#     plt.xlabel(\"Iterations\")\n",
        "#     plt.ylabel(\"Loss\")\n",
        "#     plt.show()\n",
        "\n",
        "#     plt.title(\"Training Curve\")\n",
        "#     plt.plot(iters, train_acc, label=\"Train\")\n",
        "#     plt.plot(iters, valid_acc, label=\"Validation\")\n",
        "#     plt.xlabel(\"Iterations\")\n",
        "#     plt.ylabel(\"Training Accuracy\")\n",
        "#     plt.legend(loc='best')\n",
        "#     plt.show()\n",
        "\n",
        "#     print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "#     print(\"Final Validation Accuracy: {}\".format(valid_acc[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fcFvUyjaIRP"
      },
      "outputs": [],
      "source": [
        "class CLSTM(nn.Module):\n",
        "  def __init__(self, input_size=256, hidden_size=128, num_class=62):\n",
        "    super(CLSTM, self).__init__()\n",
        "    self.name = \"CLSTM\"\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "    # Calculate the input size for fc1 dynamically\n",
        "    self.fc1 = nn.Linear(16 * 128, input_size) #Changed to calculate dynamically\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.rnn = nn.LSTM(input_size=256, hidden_size=128, bidirectional=True, num_layers=2, batch_first=True)\n",
        "    self.fc2 = nn.Linear(hidden_size*2, num_class+1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    # Calculate the input size for fc1 dynamically\n",
        "    x = x.view(batch_size, x.size(1), -1) #Changed to calculate dynamically\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc2(x)\n",
        "    # Put in required shape for to get CTC loss\n",
        "    x = x.permute(1, 0, 2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC1cMjDK5aed"
      },
      "outputs": [],
      "source": [
        "def train_CLSTM(model, train_loader, val_loader, batch_size=128, num_epochs=400, lr=1e-4):\n",
        "  drive_path = '/content/drive/MyDrive/Miniproj/Datasets/project_dataset_processed'\n",
        "  train_loader_single, val_loader_single, test_loader_single = get_data_loader(1)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  criterion = nn.CTCLoss(blank=0)\n",
        "  iters_loss, iters_acc, train_losses, valid_losses, train_accs, val_accs = [], [], [], [], [], []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      labels = labels + 1 #account for unknown\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      preds = model(images)\n",
        "      log_probs = F.log_softmax(preds, 2)\n",
        "      pred_lengths = torch.full(size=(preds.size(1),), fill_value=log_probs.size(0), dtype=torch.int32)\n",
        "      label_lengths = torch.full(size=(preds.size(1),), fill_value=labels.size(1), dtype=torch.int32)\n",
        "      loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "\n",
        "    iters_loss.append(epoch)\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    valid_loss = 0\n",
        "    for images, labels in val_loader:\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      labels = labels + 1 #account for unknown\n",
        "\n",
        "      preds = model(images)\n",
        "      log_probs = F.log_softmax(preds, 2)\n",
        "      pred_lengths = torch.full(size=(preds.size(1),), fill_value=log_probs.size(0), dtype=torch.int32)\n",
        "      label_lengths = torch.full(size=(preds.size(1),), fill_value=labels.size(1), dtype=torch.int32)\n",
        "      loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    valid_loss /= len(val_loader)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(\"Epoch: {} Train Loss: {} Valid Loss: {}\".format(epoch, train_loss, valid_loss))\n",
        "    if ((epoch + 1) % 10 == 0):\n",
        "      if epoch != 99:\n",
        "        model_path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(model.name,128,1e-4,epoch + 1)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(\"saving\")\n",
        "\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters_loss, train_losses, label=\"Train\")\n",
        "  plt.plot(iters_loss, valid_losses, label=\"Validation\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEQvEYnV9FIh"
      },
      "outputs": [],
      "source": [
        "class CLSTM(nn.Module):\n",
        "  def __init__(self, input_size=256, hidden_size=128, num_class=62):\n",
        "    super(CLSTM, self).__init__()\n",
        "    self.name = \"CLSTM\"\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "    # Remove the hardcoded input size for fc1\n",
        "    # self.fc1 = nn.Linear(16 * 128, input_size)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.rnn = nn.LSTM(input_size=256, hidden_size=128, bidirectional=True, num_layers=2, batch_first=True)\n",
        "    self.fc2 = nn.Linear(hidden_size*2, num_class+1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    # Calculate the input size for fc1 dynamically\n",
        "    x = x.view(batch_size, x.size(1), -1)\n",
        "    # Dynamically determine the input size and initialize fc1\n",
        "    input_feature_size = x.shape[2]  # Get the feature size from the reshaped tensor\n",
        "    self.fc1 = nn.Linear(input_feature_size, 256)  # Initialize fc1 with the correct input size\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc2(x)\n",
        "    # Put in required shape for to get CTC loss\n",
        "    x = x.permute(1, 0, 2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CQauYUJA-yv"
      },
      "outputs": [],
      "source": [
        "class CLSTM(nn.Module):\n",
        "  def __init__(self, input_size=256, hidden_size=128, num_class=62):\n",
        "    super(CLSTM, self).__init__()\n",
        "    self.name = \"CLSTM\"\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "\n",
        "    # Dynamically calculate the input size for the first fully connected layer (self.fc1)\n",
        "    # This is done by passing a dummy tensor through the convolutional and pooling layers\n",
        "    # Assume input image size is 70x250 (from the get_data_loader transform)\n",
        "    dummy_input = torch.randn(1, 1, 70, 250)\n",
        "    x = F.relu(self.conv1(dummy_input))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.view(x.size(0), x.size(1), -1)\n",
        "    input_feature_size = x.shape[2]\n",
        "\n",
        "    self.fc1 = nn.Linear(input_feature_size, input_size)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=True, num_layers=2, batch_first=True)\n",
        "    self.fc2 = nn.Linear(hidden_size*2, num_class+1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.view(batch_size, x.size(1), -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc2(x)\n",
        "    # Put in required shape for to get CTC loss\n",
        "    x = x.permute(1, 0, 2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UX2Vl0DA4n3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "from torch.utils import data\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow # Commented out as it's not used and might cause issues\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Ensure Google Drive is mounted if it wasn't already\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Ensure all_characters is defined as it's used in decode_label and decode_pred\n",
        "all_characters = string.ascii_letters + string.digits\n",
        "\n",
        "# Define the directory for saving models within your Google Drive\n",
        "# Adjust this path if you prefer a different location\n",
        "model_save_dir = '/content/gdrive/My Drive/Miniproj/Models'\n",
        "\n",
        "# Create the model save directory if it doesn't exist\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "  \"\"\" Generate a name for the model including the save directory \"\"\"\n",
        "  # Construct the filename\n",
        "  filename = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,batch_size,learning_rate,epoch)\n",
        "  # Combine the save directory path with the filename\n",
        "  path = os.path.join(model_save_dir, filename)\n",
        "  return path\n",
        "\n",
        "# Assuming CLSTM class is defined in a previous cell and is correct.\n",
        "# Including the definition here for completeness based on ipython-input-132\n",
        "class CLSTM(nn.Module):\n",
        "  def __init__(self, input_size=256, hidden_size=128, num_class=62):\n",
        "    super(CLSTM, self).__init__()\n",
        "    self.name = \"CLSTM\"\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "\n",
        "    # Dynamically calculate the input size for the first fully connected layer (self.fc1)\n",
        "    # This is done by passing a dummy tensor through the convolutional and pooling layers\n",
        "    # Assume input image size is 70x250 (from the get_data_loader transform)\n",
        "    dummy_input = torch.randn(1, 1, 70, 250)\n",
        "    x = F.relu(self.conv1(dummy_input))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.view(x.size(0), x.size(1), -1)\n",
        "    input_feature_size = x.shape[2]\n",
        "\n",
        "    self.fc1 = nn.Linear(input_feature_size, input_size)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=True, num_layers=2, batch_first=True)\n",
        "    self.fc2 = nn.Linear(hidden_size*2, num_class+1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.view(batch_size, x.size(1), -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc2(x)\n",
        "    # Put in required shape for to get CTC loss\n",
        "    x = x.permute(1, 0, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "# Assuming get_data_loader and decode_label functions are defined in previous cells and are correct.\n",
        "# Including placeholder definitions if needed, but ideally they should be run before this.\n",
        "def string_label(string):\n",
        "  out = []\n",
        "  index = 0\n",
        "  for char_string in string:\n",
        "    for char_index in range(0,len(all_characters)):\n",
        "      if char_string == all_characters[char_index]:\n",
        "        out.append(int(char_index))\n",
        "        break\n",
        "  return out\n",
        "\n",
        "class CAPTCHAImageDataset(data.Dataset):\n",
        "  def __init__(self, img_dir, transform=None):\n",
        "    if not os.path.exists(img_dir):\n",
        "        # Corrected the error message to show the directory path\n",
        "        raise FileNotFoundError(f\"Image directory not found: {img_dir}\")\n",
        "    self.all_images = os.listdir(img_dir)\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.all_images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if idx >= len(self.all_images):\n",
        "         raise IndexError(f\"Index {idx} out of bounds for dataset of size {len(self)}\")\n",
        "\n",
        "    img_path_relative = self.all_images[idx]\n",
        "    full_img_path = os.path.join(self.img_dir, img_path_relative)\n",
        "\n",
        "    label_filename = img_path_relative.split(\".\")[0]\n",
        "    # Corrected label extraction - assuming filename format is like 'Label-OptionalStuff.jpg' or just 'Label.jpg'\n",
        "    # This line was label = img_path.split(\"/\")[-1] -> label = img_path.split(\"-\")[0] -> label = label.replace(\".jpg\",\"\")\n",
        "    # Let's simplify to just get the part before the first hyphen or the extension.\n",
        "    parts = label_filename.split(\"-\")\n",
        "    label = parts[0]\n",
        "\n",
        "    # Corrected image loading to handle potential issues and ensure grayscale for single channel model\n",
        "    try:\n",
        "        image = Image.open(full_img_path).convert(\"L\") # Convert to grayscale\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error opening image file: {full_img_path}\")\n",
        "        # Return None or handle the error appropriately\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing image {full_img_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "\n",
        "    # Ensure labels are processed correctly for CTC Loss\n",
        "    labels_list = string_label(label)\n",
        "    if not labels_list:\n",
        "        # Handle cases where string_label returns an empty list (e.g., label has no valid characters)\n",
        "        print(f\"Warning: Could not generate labels for filename {img_path_relative}. Skipping.\")\n",
        "        return None, None\n",
        "\n",
        "    # CTC Loss typically expects a sequence of integers\n",
        "    labels = torch.Tensor(labels_list).type(torch.LongTensor)\n",
        "\n",
        "    return image, labels\n",
        "\n",
        "def get_data_loader(batch_size, is_split_data=False):\n",
        "  # Use the correct base path for the processed data\n",
        "  drive_path_dataset = '/content/gdrive/My Drive/Miniproj/DS/Processed'\n",
        "\n",
        "  transform = transforms.Compose([transforms.Resize((70,250)),transforms.ToTensor()])\n",
        "\n",
        "  # Note: is_split_data=True is intended for the Baseline CNN and uses different dimensions/data structure.\n",
        "  # This CLSTM model expects the full 70x250 image.\n",
        "  if is_split_data:\n",
        "      print(\"Warning: get_data_loader called with is_split_data=True for CLSTM model. This might not be intended.\")\n",
        "      # You might want to raise an error or adjust transforms/dataset accordingly\n",
        "      # Keeping the original transform for the full image for the CLSTM\n",
        "      pass\n",
        "\n",
        "\n",
        "  # Corrected paths to point to the subdirectories within the processed data folder\n",
        "  train_data = CAPTCHAImageDataset(os.path.join(drive_path_dataset, \"Saved_Train\"), transform=transform)\n",
        "  val_data = CAPTCHAImageDataset(os.path.join(drive_path_dataset, \"Saved_Validation\"), transform=transform)\n",
        "  test_data = CAPTCHAImageDataset(os.path.join(drive_path_dataset, \"Saved_Test\"), transform=transform)\n",
        "\n",
        "  # Custom collate function to handle variable sequence lengths for CTC loss\n",
        "  # This function filters out None items and handles padding for labels if necessary.\n",
        "  # For CTC loss, labels are typically padded, not images/features directly in the DataLoader.\n",
        "  # The CLSTM forward pass handles padding for the output sequence implicitly via the permutation.\n",
        "  # We just need to make sure the labels tensor is created correctly.\n",
        "  def collate_fn(batch):\n",
        "      # Filter out samples where loading/processing failed\n",
        "      batch = [item for item in batch if item[0] is not None and item[1] is not None]\n",
        "      if not batch:\n",
        "          return None, None # Return None if the batch is empty after filtering\n",
        "\n",
        "      images, labels = zip(*batch)\n",
        "\n",
        "      # Stack images (they should already be tensors from the transform)\n",
        "      images = torch.stack(images, 0)\n",
        "\n",
        "      # Labels need to be concatenated and provided with lengths for CTCLoss\n",
        "      label_lengths = torch.LongTensor([len(label) for label in labels])\n",
        "      labels = torch.cat(labels, 0)\n",
        "\n",
        "      return images, labels, label_lengths\n",
        "\n",
        "  # Use the custom collate function\n",
        "  train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
        "  val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "  return train_loader, val_loader, test_loader\n",
        "\n",
        "# Updated train_CLSTM function to save models in the specified directory\n",
        "def train_CLSTM(model, train_loader, val_loader, batch_size=128, num_epochs=400, lr=1e-4):\n",
        "  # Removed the single image loaders here as they are not needed for training loop\n",
        "  # drive_path = '/content/drive/MyDrive/Miniproj/Datasets/project_dataset_processed'\n",
        "  # train_loader_single, val_loader_single, test_loader_single = get_data_loader(1)\n",
        "\n",
        "  # Check GPU availability and move model\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "  print(f\"Training on: {device}\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  # CTCLoss expects blank=0 by default. Our labels were adjusted by +1, so blank should be 0.\n",
        "  criterion = nn.CTCLoss(blank=0)\n",
        "  # Removed iters_acc, train_accs, val_accs as accuracy calculation is complex for CTC\n",
        "  iters_loss, train_losses, valid_losses = [], [], []\n",
        "\n",
        "  print(\"Starting CLSTM Training...\")\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    train_loss = 0\n",
        "    # Updated data loading to match collate_fn output\n",
        "    for batch_idx, (images, labels, label_lengths) in enumerate(train_loader):\n",
        "      if images is None:\n",
        "          # Skip if the batch was empty after filtering\n",
        "          print(f\"Warning: Empty batch at epoch {epoch}, batch {batch_idx}. Skipping.\")\n",
        "          continue\n",
        "\n",
        "      images = images.to(device)\n",
        "      # Labels and label_lengths are needed by CTCLoss on the same device\n",
        "      labels = labels.to(device)\n",
        "      label_lengths = label_lengths.to(device)\n",
        "\n",
        "      # Labels are already adjusted by +1 in the collate_fn or get_item\n",
        "      # Removed the labels = labels + 1 line here\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      preds = model(images)\n",
        "      log_probs = F.log_softmax(preds, 2) # Shape: (sequence_length, batch_size, num_classes)\n",
        "      # pred_lengths should be the length of the sequence dimension for each item in the batch\n",
        "      pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device) # Ensure on device\n",
        "\n",
        "      # Check for potential issues with label_lengths being zero or exceeding sequence_length\n",
        "      if torch.any(label_lengths <= 0):\n",
        "          print(f\"Warning: Zero or negative label length detected in epoch {epoch}, batch {batch_idx}. Skipping batch.\")\n",
        "          continue\n",
        "      if torch.any(label_lengths > pred_lengths):\n",
        "          print(f\"Warning: Label length exceeds sequence length in epoch {epoch}, batch {batch_idx}. Skipping batch.\")\n",
        "          continue\n",
        "\n",
        "      try:\n",
        "          loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_loss += loss.item()\n",
        "      except Exception as e:\n",
        "          print(f\"Error during loss calculation/backward pass in epoch {epoch}, batch {batch_idx}: {e}\")\n",
        "          # Optionally, log more details or skip the batch\n",
        "          continue\n",
        "\n",
        "\n",
        "    iters_loss.append(epoch)\n",
        "    # Ensure train_loss is not divided by zero if train_loader is empty\n",
        "    if len(train_loader) > 0:\n",
        "        train_loss /= len(train_loader)\n",
        "    else:\n",
        "        train_loss = float('inf') # Or 0, depending on desired behavior for empty loader\n",
        "        print(f\"Warning: Train loader is empty in epoch {epoch}.\")\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    valid_loss = 0\n",
        "    with torch.no_grad(): # Disable gradient calculation for validation\n",
        "        # Updated data loading to match collate_fn output\n",
        "        for images, labels, label_lengths in val_loader:\n",
        "          if images is None:\n",
        "              print(f\"Warning: Empty validation batch at epoch {epoch}, batch {batch_idx}. Skipping.\")\n",
        "              continue\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          label_lengths = label_lengths.to(device)\n",
        "\n",
        "          # Labels are already adjusted by +1 in the collate_fn or get_item\n",
        "          # Removed the labels = labels + 1 line here\n",
        "\n",
        "          preds = model(images)\n",
        "          log_probs = F.log_softmax(preds, 2)\n",
        "          pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device)\n",
        "\n",
        "          if torch.any(label_lengths <= 0) or torch.any(label_lengths > pred_lengths):\n",
        "              print(f\"Warning: Invalid label/sequence length in validation epoch {epoch}, batch {batch_idx}. Skipping batch.\")\n",
        "              continue\n",
        "\n",
        "          try:\n",
        "              loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "              valid_loss += loss.item()\n",
        "          except Exception as e:\n",
        "              print(f\"Error during validation loss calculation in epoch {epoch}, batch {batch_idx}: {e}\")\n",
        "              continue\n",
        "\n",
        "    # Ensure valid_loss is not divided by zero if val_loader is empty\n",
        "    if len(val_loader) > 0:\n",
        "        valid_loss /= len(val_loader)\n",
        "    else:\n",
        "        valid_loss = float('inf') # Or 0\n",
        "        print(f\"Warning: Validation loader is empty in epoch {epoch}.\")\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "\n",
        "    print(\"Epoch: {} Train Loss: {:.4f} Valid Loss: {:.4f}\".format(epoch + 1, train_loss, valid_loss))\n",
        "\n",
        "    # Save the model every 10 epochs\n",
        "    if ((epoch + 1) % 10 == 0):\n",
        "      # Corrected epoch number in the filename to match the loop iteration\n",
        "      model_path = get_model_name(model.name, batch_size, lr, epoch + 1)\n",
        "      # Ensure the directory exists before saving (redundant if os.makedirs was called earlier, but good practice)\n",
        "      # os.makedirs(os.path.dirname(model_path), exist_ok=True) # Handled by get_model_name + initial os.makedirs\n",
        "      torch.save(model.state_dict(), model_path)\n",
        "      print(f\"Saved model to {model_path}\")\n",
        "\n",
        "  print('Finished Training')\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "\n",
        "  # plotting\n",
        "  plt.figure() # Create a new figure for the plot\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters_loss, train_losses, label=\"Train Loss\") # Changed label to Loss\n",
        "  plt.plot(iters_loss, valid_losses, label=\"Validation Loss\") # Changed label to Loss\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  # Accuracy plotting requires a separate function for CTC decoding, which is not directly calculated here.\n",
        "  # The original accuracy calculation function `get_accuracy` is for classification, not sequence prediction with CTC.\n",
        "  # You would need to implement a CTC beam search or greedy decoding function to get accuracy.\n",
        "\n",
        "# Assuming the original `get_accuracy` is for the Baseline CNN and not applicable here.\n",
        "# Including a placeholder for accuracy plotting if you implement decoding later.\n",
        "# def plot_accuracy(epochs, train_accs, val_accs):\n",
        "#     plt.figure()\n",
        "#     plt.title(\"Accuracy Curve\")\n",
        "#     plt.plot(epochs, train_accs, label=\"Train Accuracy\")\n",
        "#     plt.plot(epochs, val_accs, label=\"Validation Accuracy\")\n",
        "#     plt.xlabel(\"Epochs\")\n",
        "#     plt.ylabel(\"Accuracy\")\n",
        "#     plt.legend(loc='best')\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "def decode_pred(pred):\n",
        "    # Greedy decoding for simplicity\n",
        "    # pred shape is (sequence_length, batch_size, num_classes+1)\n",
        "    # Take the argmax along the class dimension\n",
        "    _, max_preds = torch.max(pred, dim=2) # Shape: (sequence_length, batch_size)\n",
        "\n",
        "    decoded_preds = []\n",
        "    for batch_idx in range(max_preds.size(1)):\n",
        "        # Get the sequence for this item in the batch\n",
        "        sequence = max_preds[:, batch_idx].tolist()\n",
        "\n",
        "        decoded_sequence = []\n",
        "        last_char = None\n",
        "        for char_code in sequence:\n",
        "            # Apply CTC decoding rules: remove consecutive duplicates and blanks (code 0)\n",
        "            if char_code != last_char and char_code != 0:\n",
        "                # Map the character code back to the original character index (subtract 1)\n",
        "                # Then use that index to get the character from all_characters\n",
        "                if 0 < char_code <= len(all_characters): # Check bounds\n",
        "                    decoded_sequence.append(all_characters[char_code - 1])\n",
        "                else:\n",
        "                    # Handle unexpected character codes if necessary\n",
        "                    pass\n",
        "            last_char = char_code\n",
        "        decoded_preds.append(\"\".join(decoded_sequence))\n",
        "    return decoded_preds\n",
        "\n",
        "def decode_label(label_tensor):\n",
        "  # label_tensor is expected to be a 1D tensor of shape (label_length,)\n",
        "  decoded_label = []\n",
        "  for char_code in label_tensor.tolist():\n",
        "      # Map the character code back to the original character index (subtract 1)\n",
        "      # Then use that index to get the character from all_characters\n",
        "      if 0 < char_code <= len(all_characters): # Check bounds and skip blank (0)\n",
        "          decoded_label.append(all_characters[char_code - 1])\n",
        "      else:\n",
        "          # Handle unexpected character codes if necessary (shouldn't happen with valid labels)\n",
        "          pass\n",
        "\n",
        "  return \"\".join(decoded_label)\n",
        "\n",
        "\n",
        "# Updated generate_graph_data to load from the correct path and use CTCLoss for accuracy\n",
        "def generate_graph_data():\n",
        "  epochs = []\n",
        "  # We'll collect losses here as accuracy is complex with CTC\n",
        "  train_losses_hist, val_losses_hist = [], []\n",
        "\n",
        "  # Add initial point (epoch 0) if needed, but losses/acc will be 0/high\n",
        "  # epochs.append(0)\n",
        "  # train_losses_hist.append(0.0) # Or a placeholder\n",
        "  # val_losses_hist.append(0.0) # Or a placeholder\n",
        "\n",
        "  # Define the same batch size used for training\n",
        "  batch_size_for_eval = 128 # Or whatever batch size you used for train_CLSTM\n",
        "\n",
        "  # Get loaders for evaluation. No need for single loaders here.\n",
        "  # Using batch_size_for_eval for more efficient loading\n",
        "  train_loader_eval, val_loader_eval, test_loader_eval = get_data_loader(batch_size_for_eval)\n",
        "\n",
        "  # Check GPU availability and move model\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # Initialize a model instance outside the loop\n",
        "  model = CLSTM()\n",
        "  model.to(device)\n",
        "  criterion = nn.CTCLoss(blank=0)\n",
        "\n",
        "  print(\"Generating graph data from saved models...\")\n",
        "\n",
        "  # Iterate through the epochs where models were saved (every 10 epochs)\n",
        "  for i in range(10, 401, 10): # Go up to and including epoch 400\n",
        "    # Construct the model path using the updated get_model_name\n",
        "    model_path = get_model_name(model.name, 128, 1e-4, i) # Use the same parameters as training\n",
        "\n",
        "    # Check if the model file exists before trying to load it\n",
        "    if not os.path.exists(model_path):\n",
        "      print(f\"Warning: Model file not found for epoch {i} at {model_path}. Skipping.\")\n",
        "      continue # Skip this epoch if the file doesn't exist\n",
        "\n",
        "    try:\n",
        "        # Load the model state dictionary\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.eval() # Set model to evaluation mode\n",
        "\n",
        "        epochs.append(i)\n",
        "\n",
        "        # Calculate average training loss for this epoch\n",
        "        train_loss_epoch = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, labels, label_lengths) in enumerate(train_loader_eval):\n",
        "                if images is None: continue\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                label_lengths = label_lengths.to(device)\n",
        "                if torch.any(label_lengths <= 0) or torch.any(label_lengths > torch.full(size=(images.size(0),), fill_value=images.size(3) // (2*2), dtype=torch.int32).to(device)): continue\n",
        "                preds = model(images)\n",
        "                log_probs = F.log_softmax(preds, 2)\n",
        "                pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device)\n",
        "                loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "                train_loss_epoch += loss.item()\n",
        "            if len(train_loader_eval) > 0:\n",
        "                train_loss_epoch /= len(train_loader_eval)\n",
        "            else:\n",
        "                train_loss_epoch = float('inf')\n",
        "        train_losses_hist.append(train_loss_epoch)\n",
        "\n",
        "\n",
        "        # Calculate average validation loss for this epoch\n",
        "        val_loss_epoch = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, labels, label_lengths) in enumerate(val_loader_eval):\n",
        "                if images is None: continue\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                label_lengths = label_lengths.to(device)\n",
        "                if torch.any(label_lengths <= 0) or torch.any(label_lengths > torch.full(size=(images.size(0),), fill_value=images.size(3) // (2*2), dtype=torch.int32).to(device)): continue\n",
        "                preds = model(images)\n",
        "                log_probs = F.log_softmax(preds, 2)\n",
        "                pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device)\n",
        "                loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "                val_loss_epoch += loss.item()\n",
        "            if len(val_loader_eval) > 0:\n",
        "                val_loss_epoch /= len(val_loader_eval)\n",
        "            else:\n",
        "                val_loss_epoch = float('inf')\n",
        "        val_losses_hist.append(val_loss_epoch)\n",
        "\n",
        "        # Note: Calculating accuracy for CTC requires decoding, which is not handled by get_accuracy.\n",
        "        # If you implement a decoding function, you would calculate accuracy here.\n",
        "        # For now, the graph will show loss.\n",
        "\n",
        "        print(f\"Processed epoch {i}: Train Loss = {train_loss_epoch:.4f}, Validation Loss = {val_loss_epoch:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or evaluating model for epoch {i} from {model_path}: {e}\")\n",
        "        # Optionally, log more details or skip this epoch's data points\n",
        "        continue\n",
        "\n",
        "\n",
        "  # Plotting the loss curve\n",
        "  plt.figure() # Create a new figure for the plot\n",
        "  plt.title(\"Training and Validation Loss Curve\")\n",
        "  plt.plot(epochs, train_losses_hist, label=\"Train Loss\")\n",
        "  plt.plot(epochs, val_losses_hist, label=\"Validation Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  # If you calculate accuracy later, you can add another plotting section here.\n",
        "  # print(\"\\nEpochs recorded: {}\".format(epochs))\n",
        "  # print(\"Train Losses recorded: {}\".format(train_losses_hist))\n",
        "  # print(\"Validation Losses recorded: {}\".format(val_losses_hist))\n",
        "\n",
        "# Example usage for training the CLSTM (assuming data loaders are ready)\n",
        "# drive_path_processed_data = '/content/drive/MyDrive/Miniproj/DS/Processed'\n",
        "# train_loader, val_loader, test_loader = get_data_loader(128) # Use the same batch size as in train_CLSTM\n",
        "# model = CLSTM() # Initialize the model\n",
        "# train_CLSTM(model, train_loader, val_loader, batch_size=128, num_epochs=400, lr=1e-4)\n",
        "\n",
        "\n",
        "# Call generate_graph_data after training and saving models\n",
        "generate_graph_data()\n",
        "\n",
        "# Example usage for decoding a single image after training (requires loading the model)\n",
        "# This part was in a commented out cell, adding it here for completeness if you want to test inference\n",
        "# drive_path_processed_data = '/content/drive/MyDrive/Miniproj/DS/Processed'\n",
        "# train_loader_single, val_loader_single, test_loader_single = get_data_loader(1) # Get a loader with batch size 1\n",
        "\n",
        "# # Load the best performing model (e.g., epoch 400 based on your training)\n",
        "# model = CLSTM()\n",
        "# model_filename_final = get_model_name(model.name, 128, 1e-4, 400) # Use the final epoch number\n",
        "# model_path_final = os.path.join(model_save_dir, model_filename_final)\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#     model.load_state_dict(torch.load(model_path_final))\n",
        "#     model.cuda()\n",
        "# else:\n",
        "#     model.load_state_dict(torch.load(model_path_final, map_location='cpu'))\n",
        "\n",
        "# model.eval()\n",
        "\n",
        "# print(\"\\nDemonstrating inference on a test image:\")\n",
        "# with torch.no_grad():\n",
        "#     # Get one image from the test loader\n",
        "#     for images, labels, label_lengths in test_loader_single:\n",
        "#         if images is None: continue # Skip empty batch\n",
        "#         # Move image and labels to the same device as the model\n",
        "#         device = next(model.parameters()).device # Get model's device\n",
        "#         images = images.to(device)\n",
        "#         labels = labels.to(device) # Move labels to device for decode_label if needed\n",
        "#         label_lengths = label_lengths.to(device) # Move label_lengths to device\n",
        "\n",
        "#         # Forward pass\n",
        "#         preds = model(images) # Shape: (sequence_length, batch_size=1, num_classes+1)\n",
        "\n",
        "#         # Decode the prediction\n",
        "#         dec_pred = decode_pred(preds)\n",
        "\n",
        "#         # Decode the ground truth label\n",
        "#         # Since labels is a concatenated tensor, you need to extract the single label tensor for this batch\n",
        "#         # Assuming batch size is 1, labels is a 1D tensor of shape (label_length,)\n",
        "#         dec_label = decode_label(labels)\n",
        "\n",
        "#         print(f\"Predicted: {dec_pred[0]}\") # dec_pred is a list of strings\n",
        "#         print(f\"Ground Truth: {dec_label}\")\n",
        "\n",
        "#         # Optional: Display the image\n",
        "#         # You need a function to display the tensor image\n",
        "#         # Assuming show_image is defined elsewhere and can handle the tensor format\n",
        "#         # show_image(images.cpu().squeeze(0)) # Move back to CPU for plotting\n",
        "\n",
        "#         break # Process only the first image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSUqfIFRG8MX"
      },
      "outputs": [],
      "source": [
        "train_accs = [0.0, 0.0, 0.009859872611464968, 0.010789808917197453, 0.013363057324840765, 0.015031847133757962, 0.03211464968152866, 0.05419108280254777, 0.08047133757961783, 0.09649681528662421, 0.09005095541401274, 0.14239490445859873, 0.1699235668789809, 0.20080254777070064, 0.23828025477707007, 0.27845859872611467, 0.31142675159235667, 0.3516050955414013, 0.39038216560509553, 0.4108152866242038, 0.4487388535031847]\n",
        "val_accs = [0.0, 0.0, 0.010372931588046431, 0.008446530007409237, 0.010323536675722401, 0.014028155100024697, 0.027315386515188937, 0.04687577179550506, 0.0704865398863917, 0.08446530007409236, 0.07942701901704124, 0.1259570264262781, 0.1503581131143492, 0.172585823660163, 0.20617436404050382, 0.23378612002963695, 0.2609533218078538, 0.29928377377130155, 0.3268955297604347, 0.3475919980242035, 0.37831563348975056]\n",
        "\n",
        "# model = model.cuda()\n",
        "# for images, labels in test_loader_single:\n",
        "#   images = images.cuda()\n",
        "#   pred = model(images)\n",
        "#   dec_pred = decode_pred(pred)\n",
        "#   dec_label = decode_labels(labels)\n",
        "\n",
        "#   print(\"Pred: {}\".format(dec_pred))\n",
        "#   print(\"Label: {}\".format(dec_label))\n",
        "\n",
        "def decode_label(label):\n",
        "  decoded_label = []\n",
        "  for char in label:\n",
        "    decoded_label.append(all_characters[int(char)])\n",
        "\n",
        "  return decoded_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTo_w2fmjoaE"
      },
      "source": [
        "Data Demonstration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7DLpzVdfrvV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# from torchvision.io import read_image # Not used\n",
        "from torch.utils import data\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow # Commented out as it's not used and might cause issues\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Ensure Google Drive is mounted if it wasn't already\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Ensure all_characters is defined as it's used in decode_label and decode_pred\n",
        "all_characters = string.ascii_letters + string.digits\n",
        "\n",
        "# Define the directory for saving models within your Google Drive\n",
        "# Adjust this path if you prefer a different location\n",
        "model_save_dir = '/content/gdrive/My Drive/Miniproj/Models'\n",
        "\n",
        "# Create the model save directory if it doesn't exist\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "  \"\"\" Generate a name for the model including the save directory \"\"\"\n",
        "  # Construct the filename\n",
        "  filename = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,batch_size,learning_rate,epoch)\n",
        "  # Combine the save directory path with the filename\n",
        "  # Ensure the directory exists before returning the path (can be done here or before saving)\n",
        "  # os.makedirs(model_save_dir, exist_ok=True) # Already done outside this function\n",
        "  path = os.path.join(model_save_dir, filename)\n",
        "  return path\n",
        "\n",
        "# Assuming CLSTM class is defined in a previous cell and is correct.\n",
        "# Including the definition here for completeness based on ipython-input-132\n",
        "class CLSTM(nn.Module):\n",
        "  def __init__(self, input_size=256, hidden_size=128, num_class=62):\n",
        "    super(CLSTM, self).__init__()\n",
        "    self.name = \"CLSTM\"\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "\n",
        "    # Dynamically calculate the input size for the first fully connected layer (self.fc1)\n",
        "    # This is done by passing a dummy tensor through the convolutional and pooling layers\n",
        "    # Assume input image size is 70x250 (from the get_data_loader transform)\n",
        "    dummy_input = torch.randn(1, 1, 70, 250)\n",
        "    x = F.relu(self.conv1(dummy_input))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.view(x.size(0), x.size(1), -1)\n",
        "    input_feature_size = x.shape[2]\n",
        "\n",
        "    self.fc1 = nn.Linear(input_feature_size, input_size)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=True, num_layers=2, batch_first=True)\n",
        "    self.fc2 = nn.Linear(hidden_size*2, num_class+1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.view(batch_size, x.size(1), -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc2(x)\n",
        "    # Put in required shape for to get CTC loss\n",
        "    x = x.permute(1, 0, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "# Assuming string_label function is defined in a previous cell and is correct.\n",
        "# Including placeholder definition\n",
        "def string_label(string):\n",
        "  out = []\n",
        "  # Using a loop to map characters to indices based on all_characters\n",
        "  char_to_index = {char: i for i, char in enumerate(all_characters)}\n",
        "  for char_string in string:\n",
        "      if char_string in char_to_index:\n",
        "          # CTC blank is often 0. Labels should start from 1.\n",
        "          # So, map char index (0 to 61) to label (1 to 62).\n",
        "          out.append(char_to_index[char_string] + 1)\n",
        "      else:\n",
        "          # Handle characters not in all_characters if necessary\n",
        "          # print(f\"Warning: Character '{char_string}' not found in all_characters.\")\n",
        "          pass # Or append a specific value to indicate unknown character if needed\n",
        "\n",
        "  return out\n",
        "\n",
        "\n",
        "class CAPTCHAImageDataset(data.Dataset):\n",
        "  def __init__(self, img_dir, transform=None):\n",
        "    if not os.path.exists(img_dir):\n",
        "        # Corrected the error message to show the directory path\n",
        "        raise FileNotFoundError(f\"Image directory not found: {img_dir}\")\n",
        "    # Filter out directories and non-image files if necessary, though os.listdir is usually fine\n",
        "    self.all_images = [f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.all_images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if idx >= len(self.all_images):\n",
        "         raise IndexError(f\"Index {idx} out of bounds for dataset of size {len(self)}\")\n",
        "\n",
        "    img_path_relative = self.all_images[idx]\n",
        "    full_img_path = os.path.join(self.img_dir, img_path_relative)\n",
        "\n",
        "    # Extract label from filename\n",
        "    label_filename = os.path.splitext(img_path_relative)[0] # Get filename without extension\n",
        "    # Assuming filename format is 'Label-OptionalStuff.jpg' or just 'Label.jpg'\n",
        "    # Take the part before the first hyphen or the whole name if no hyphen\n",
        "    parts = label_filename.split(\"-\")\n",
        "    label = parts[0]\n",
        "\n",
        "    # Corrected image loading to handle potential issues and ensure grayscale for single channel model\n",
        "    try:\n",
        "        # Convert to grayscale ('L') to match the model's single input channel\n",
        "        image = Image.open(full_img_path).convert(\"L\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error opening image file: {full_img_path}\")\n",
        "        # Return None or handle the error appropriately\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing image {full_img_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "\n",
        "    # Ensure labels are processed correctly for CTC Loss\n",
        "    labels_list = string_label(label) # This now returns indices + 1\n",
        "    if not labels_list:\n",
        "        # Handle cases where string_label returns an empty list (e.g., label has no valid characters)\n",
        "        # Or if the extracted label string was empty\n",
        "        print(f\"Warning: Could not generate labels for filename {img_path_relative} (label: '{label}'). Skipping.\")\n",
        "        return None, None\n",
        "\n",
        "    # CTC Loss typically expects a sequence of integers. string_label already returns a list of ints.\n",
        "    # Convert the list to a LongTensor\n",
        "    labels = torch.LongTensor(labels_list)\n",
        "\n",
        "\n",
        "    return image, labels\n",
        "\n",
        "# Updated get_data_loader function to use the custom collate function\n",
        "def get_data_loader(batch_size, is_split_data=False):\n",
        "  # Use the correct base path for the processed data\n",
        "  drive_path_dataset = '/content/gdrive/My Drive/Miniproj/DS/Processed'\n",
        "\n",
        "  transform = transforms.Compose([transforms.Resize((70,250)),transforms.ToTensor()])\n",
        "\n",
        "  # Note: is_split_data=True is intended for the Baseline CNN and uses different dimensions/data structure.\n",
        "  # This CLSTM model expects the full 70x250 image.\n",
        "  if is_split_data:\n",
        "      print(\"Warning: get_data_loader called with is_split_data=True for CLSTM model. This might not be intended.\")\n",
        "      # Keeping the original transform for the full image for the CLSTM\n",
        "      pass\n",
        "\n",
        "\n",
        "  # Corrected paths to point to the subdirectories within the processed data folder\n",
        "  train_data = CAPTCHAImageDataset(os.path.join(drive_path_dataset, \"Saved_Train\"), transform=transform)\n",
        "  val_data = CAPTCHAImageDataset(os.path.join(drive_path_dataset, \"Saved_Validation\"), transform=transform)\n",
        "  test_data = CAPTCHAImageDataset(os.path.join(drive_path_dataset, \"Saved_Test\"), transform=transform)\n",
        "\n",
        "  # Custom collate function to handle variable sequence lengths for CTC loss\n",
        "  def collate_fn(batch):\n",
        "      # Filter out samples where loading/processing failed (returned None, None)\n",
        "      batch = [item for item in batch if item is not None and item[0] is not None and item[1] is not None]\n",
        "      if not batch:\n",
        "          return None, None, None # Return None for all outputs if the batch is empty after filtering\n",
        "\n",
        "      # Separate images and labels\n",
        "      images, labels = zip(*batch)\n",
        "\n",
        "      # Stack images (they should already be tensors from the transform)\n",
        "      images = torch.stack(images, 0)\n",
        "\n",
        "      # Labels need to be concatenated and provided with lengths for CTCLoss\n",
        "      # labels is a tuple of LongTensors from string_label\n",
        "      label_lengths = torch.LongTensor([len(label) for label in labels])\n",
        "      labels = torch.cat(labels, 0) # Concatenate all individual label tensors into one long tensor\n",
        "\n",
        "      return images, labels, label_lengths\n",
        "\n",
        "  # Use the custom collate function\n",
        "  train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn, pin_memory=True if torch.cuda.is_available() else False)\n",
        "  val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn, pin_memory=True if torch.cuda.is_available() else False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn, pin_memory=True if torch.cuda.is_available() else False)\n",
        "\n",
        "\n",
        "  return train_loader, val_loader, test_loader\n",
        "\n",
        "# Updated train_CLSTM function\n",
        "def train_CLSTM(model, train_loader, val_loader, batch_size=128, num_epochs=400, lr=1e-4):\n",
        "\n",
        "  # Check GPU availability and move model\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "  print(f\"Training on: {device}\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  # CTCLoss expects blank=0. Our labels are 1-62. So blank is 0.\n",
        "  criterion = nn.CTCLoss(blank=0, reduction='mean') # Use mean reduction for average loss per batch\n",
        "\n",
        "  iters_loss, train_losses, valid_losses = [], [], []\n",
        "\n",
        "  print(\"Starting CLSTM Training...\")\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    train_loss = 0\n",
        "    # Updated data loading to match collate_fn output: images, labels, label_lengths\n",
        "    # Use enumerate to get batch_idx if needed for logging\n",
        "    for batch_idx, batch_data in enumerate(train_loader):\n",
        "      images, labels, label_lengths = batch_data # Unpack the tuple from collate_fn\n",
        "\n",
        "      if images is None:\n",
        "          # Skip if the batch was empty after filtering in collate_fn\n",
        "          print(f\"Warning: Empty batch at epoch {epoch}, batch {batch_idx}. Skipping.\")\n",
        "          continue\n",
        "\n",
        "      images = images.to(device)\n",
        "      # Labels and label_lengths are needed by CTCLoss on the same device\n",
        "      labels = labels.to(device)\n",
        "      label_lengths = label_lengths.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      preds = model(images) # Shape: (sequence_length, batch_size, num_classes+1)\n",
        "\n",
        "      log_probs = F.log_softmax(preds, 2) # Shape: (sequence_length, batch_size, num_classes+1)\n",
        "      # pred_lengths should be the length of the sequence dimension for each item in the batch\n",
        "      # This is the first dimension of log_probs, repeated batch_size times\n",
        "      pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device) # Ensure on device\n",
        "\n",
        "      # Check for potential issues with label_lengths or pred_lengths\n",
        "      # CTCLoss requires label_lengths <= pred_lengths\n",
        "      if torch.any(label_lengths <= 0):\n",
        "           # This should ideally be handled in string_label or dataset __getitem__\n",
        "           print(f\"Warning: Zero or negative label length detected in epoch {epoch}, batch {batch_idx}. Skipping batch.\")\n",
        "           continue\n",
        "      if torch.any(label_lengths > pred_lengths):\n",
        "           # This can happen if image processing/transforms lead to very short sequences\n",
        "           # or if labels are unexpectedly long. The CLSTM architecture output sequence length\n",
        "           # depends on the convolutional/pooling layers and image width.\n",
        "           # For 70x250 input, Conv(3)+Pool(2)+Conv(3)+Pool(2) results in output width:\n",
        "           # (250 - 3 + 1)/1 = 248 -> 248/2 = 124 -> (124 - 3 + 1)/1 = 122 -> 122/2 = 61\n",
        "           # The sequence length is 61. If any label is > 61, CTCLoss will error.\n",
        "           print(f\"Warning: Label length exceeds sequence length in epoch {epoch}, batch {batch_idx}. Max label length is {label_lengths.max().item()}, sequence length is {pred_lengths.max().item()}. Skipping batch.\")\n",
        "           continue\n",
        "\n",
        "\n",
        "      try:\n",
        "          # CTCLoss(log_probs, targets, input_lengths, target_lengths)\n",
        "          loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "          loss.backward()\n",
        "          # Gradient clipping to prevent exploding gradients, common in RNNs/LSTMs\n",
        "          # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "          optimizer.step()\n",
        "          train_loss += loss.item() # Accumulate the mean loss for the batch\n",
        "      except Exception as e:\n",
        "          print(f\"Error during loss calculation/backward pass in epoch {epoch}, batch {batch_idx}: {e}\")\n",
        "          # Optionally, log more details or skip the batch\n",
        "          continue # Skip to the next batch if an error occurs\n",
        "\n",
        "    iters_loss.append(epoch)\n",
        "    # Ensure train_loss is not divided by zero if train_loader is empty or all batches skipped\n",
        "    if len(train_loader) > 0 and (batch_idx + 1) > 0: # Check if at least one batch was processed\n",
        "        # Calculate average loss over processed batches\n",
        "        # train_loss has accumulated the mean loss per batch, so divide by number of processed batches\n",
        "        avg_train_loss = train_loss / (batch_idx + 1) # Use batch_idx + 1 as counter\n",
        "    else:\n",
        "        avg_train_loss = float('inf') # Or 0, depending on desired behavior for empty loader\n",
        "        if len(train_loader) == 0:\n",
        "            print(f\"Warning: Train loader is empty in epoch {epoch}. No training occurred.\")\n",
        "        else:\n",
        "             print(f\"Warning: All batches skipped in train loader in epoch {epoch}. No training occurred.\")\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    valid_loss = 0\n",
        "    num_valid_batches = 0\n",
        "    with torch.no_grad(): # Disable gradient calculation for validation\n",
        "        # Updated data loading to match collate_fn output\n",
        "        for batch_idx, batch_data in enumerate(val_loader):\n",
        "          images, labels, label_lengths = batch_data\n",
        "\n",
        "          if images is None:\n",
        "              print(f\"Warning: Empty validation batch at epoch {epoch}, batch {batch_idx}. Skipping.\")\n",
        "              continue\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          label_lengths = label_lengths.to(device)\n",
        "\n",
        "          if torch.any(label_lengths <= 0):\n",
        "              print(f\"Warning: Invalid label length in validation epoch {epoch}, batch {batch_idx}. Skipping batch.\")\n",
        "              continue\n",
        "          # Re-calculate sequence length for safety, though it should be consistent\n",
        "          sequence_length = images.size(3) // (2 * 2) # Assuming 2 conv+pool steps with kernel/stride 2\n",
        "          if torch.any(label_lengths > torch.full(size=(images.size(0),), fill_value=sequence_length, dtype=torch.int32).to(device)):\n",
        "               print(f\"Warning: Label length exceeds sequence length in validation epoch {epoch}, batch {batch_idx}. Skipping batch.\")\n",
        "               continue\n",
        "\n",
        "\n",
        "          try:\n",
        "              preds = model(images)\n",
        "              log_probs = F.log_softmax(preds, 2)\n",
        "              pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device)\n",
        "              loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "              valid_loss += loss.item() # Accumulate the mean loss for the batch\n",
        "              num_valid_batches += 1\n",
        "          except Exception as e:\n",
        "              print(f\"Error during validation loss calculation in epoch {epoch}, batch {batch_idx}: {e}\")\n",
        "              continue # Skip to the next batch\n",
        "\n",
        "\n",
        "    # Ensure valid_loss is not divided by zero if val_loader is empty or all batches skipped\n",
        "    if num_valid_batches > 0:\n",
        "        avg_valid_loss = valid_loss / num_valid_batches # Average over processed batches\n",
        "    else:\n",
        "        avg_valid_loss = float('inf') # Or 0\n",
        "        if len(val_loader) == 0:\n",
        "            print(f\"Warning: Validation loader is empty in epoch {epoch}. No validation occurred.\")\n",
        "        else:\n",
        "            print(f\"Warning: All batches skipped in validation loader in epoch {epoch}. No validation occurred.\")\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "\n",
        "\n",
        "    print(\"Epoch: {} Train Loss: {:.4f} Valid Loss: {:.4f}\".format(epoch + 1, avg_train_loss, avg_valid_loss))\n",
        "\n",
        "    # Save the model every 10 epochs\n",
        "    if ((epoch + 1) % 10 == 0):\n",
        "      # Corrected epoch number in the filename to match the loop iteration\n",
        "      model_path = get_model_name(model.name, batch_size, lr, epoch + 1) # Use actual batch_size and lr\n",
        "      torch.save(model.state_dict(), model_path)\n",
        "      print(f\"Saved model to {model_path}\")\n",
        "\n",
        "  print('Finished Training')\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "\n",
        "  # plotting\n",
        "  plt.figure() # Create a new figure for the plot\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters_loss, train_losses, label=\"Train Loss\") # Changed label to Loss\n",
        "  plt.plot(iters_loss, valid_losses, label=\"Validation Loss\") # Changed label to Loss\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  # Accuracy plotting requires a separate function for CTC decoding, which is not directly calculated here.\n",
        "  # The original accuracy calculation function `get_accuracy` is for classification, not sequence prediction with CTC.\n",
        "  # You would need to implement a CTC beam search or greedy decoding function to get accuracy.\n",
        "\n",
        "\n",
        "def decode_pred(pred_tensor, all_characters, blank_idx=0):\n",
        "    # pred_tensor is model output, shape (sequence_length, batch_size, num_classes+1)\n",
        "    # For batch_size=1, shape is (sequence_length, 1, num_classes+1)\n",
        "    if pred_tensor.ndim == 3 and pred_tensor.shape[1] == 1:\n",
        "        # Squeeze the batch dimension if it's 1\n",
        "        pred_tensor = pred_tensor.squeeze(1) # Shape becomes (sequence_length, num_classes+1)\n",
        "    elif pred_tensor.ndim != 2:\n",
        "         # This case might occur if batch size > 1 is passed\n",
        "         # For decoding, usually process batch items one by one or handle batched decoding.\n",
        "         # This function is currently designed for a single prediction sequence.\n",
        "         raise ValueError(f\"decode_pred expects tensor shape (seq_len, num_classes+1) or (seq_len, 1, num_classes+1), but got {pred_tensor.shape}\")\n",
        "\n",
        "\n",
        "    # Use argmax to get the predicted class index for each time step\n",
        "    argmax_preds = torch.argmax(pred_tensor, dim=1).cpu().numpy()\n",
        "\n",
        "    decoded_chars = []\n",
        "    # Simple greedy decoding: remove consecutive duplicates and blanks\n",
        "    last_decoded_char = None # Store the last character added to the decoded string\n",
        "\n",
        "    for idx in argmax_preds:\n",
        "        if idx != blank_idx: # If the current prediction is not the blank symbol (0)\n",
        "            # Adjust index for all_characters if blank_idx is 0\n",
        "            # Our string_label adds 1, so indices in pred are 1-62 for chars, 0 for blank.\n",
        "            # To get the original index in all_characters (0-61), subtract 1.\n",
        "            char_index = idx - 1\n",
        "\n",
        "            if 0 <= char_index < len(all_characters):\n",
        "                 current_char = all_characters[char_index]\n",
        "                 # Append the character if it's the first character OR\n",
        "                 # it's different from the last non-blank character added\n",
        "                 if not decoded_chars or current_char != last_decoded_char:\n",
        "                      decoded_chars.append(current_char)\n",
        "                      last_decoded_char = current_char # Update the last added character\n",
        "            else:\n",
        "                 # This case indicates an issue with model output if indices are out of expected range\n",
        "                 print(f\"Warning: decode_pred encountered out-of-bounds character index: {char_index} (from predicted index {idx})\")\n",
        "                 pass # Ignore invalid index\n",
        "\n",
        "        else: # If the current prediction IS the blank symbol (0)\n",
        "            # Skip blank symbols in the output\n",
        "            pass\n",
        "\n",
        "\n",
        "    return \"\".join(decoded_chars) # Join characters into a string\n",
        "\n",
        "def decode_label(label_tensor, all_characters):\n",
        "  # label_tensor is expected to be a 1D LongTensor of shape (label_length,)\n",
        "  # Labels were encoded with +1 (1 to 62). Blank is 0 but should not appear in true labels.\n",
        "  decoded_label = []\n",
        "  for char_code in label_tensor.tolist(): # Convert tensor to list of integers\n",
        "      # Map the character code back to the original character index (subtract 1)\n",
        "      char_index_adjusted = int(char_code) - 1 # Subtract 1 to get 0-61 index\n",
        "      # Then use that index to get the character from all_characters\n",
        "      if 0 <= char_index_adjusted < len(all_characters): # Check bounds\n",
        "          decoded_label.append(all_characters[char_index_adjusted])\n",
        "      else:\n",
        "          # This case shouldn't happen for true labels if encoding is correct\n",
        "          print(f\"Warning: decode_label encountered out-of-bounds character index: {char_index_adjusted} (from label code {char_code})\")\n",
        "          decoded_label.append(\"?\") # Use a placeholder for unknown index\n",
        "\n",
        "  return \"\".join(decoded_label)\n",
        "\n",
        "\n",
        "# Updated generate_graph_data to load from the correct path and plot loss\n",
        "def generate_graph_data():\n",
        "  epochs = []\n",
        "  # We'll collect losses here as accuracy calculation with CTC requires decoding\n",
        "  train_losses_hist, val_losses_hist = [], []\n",
        "\n",
        "  # Define the same batch size used for training for consistent evaluation\n",
        "  batch_size_for_eval = 128 # Or whatever batch size you used for train_CLSTM\n",
        "\n",
        "  # Get loaders for evaluation. Use the batch size for eval.\n",
        "  train_loader_eval, val_loader_eval, test_loader_eval = get_data_loader(batch_size_for_eval)\n",
        "\n",
        "  # Check GPU availability and move model\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # Initialize a model instance outside the loop\n",
        "  model = CLSTM()\n",
        "  model.to(device)\n",
        "  criterion = nn.CTCLoss(blank=0, reduction='mean') # Use mean reduction for consistency\n",
        "\n",
        "  print(\"Generating graph data from saved models...\")\n",
        "\n",
        "  # Iterate through the epochs where models were saved (every 10 epochs)\n",
        "  # Adjust the range based on your train_CLSTM save logic (e.g., range(10, num_epochs + 1, 10))\n",
        "  num_epochs_trained = 400 # Assuming 400 epochs were trained and saved every 10\n",
        "  for i in range(10, num_epochs_trained + 1, 10): # Go up to and including the last epoch saved\n",
        "    # Construct the model path using the updated get_model_name\n",
        "    model_path = get_model_name(model.name, 128, 1e-4, i) # Use the same parameters as training\n",
        "\n",
        "    # Check if the model file exists before trying to load it\n",
        "    if not os.path.exists(model_path):\n",
        "      print(f\"Warning: Model file not found for epoch {i} at {model_path}. Skipping.\")\n",
        "      continue # Skip this epoch if the file doesn't exist\n",
        "    elif os.path.isdir(model_path):\n",
        "         print(f\"Warning: Path {model_path} is a directory, not a file. Skipping epoch {i}.\")\n",
        "         continue # Skip if it's a directory\n",
        "\n",
        "    try:\n",
        "        # Load the model state dictionary\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.eval() # Set model to evaluation mode\n",
        "\n",
        "        epochs.append(i)\n",
        "\n",
        "        # Calculate average training loss for this epoch\n",
        "        train_loss_epoch = 0\n",
        "        num_train_batches_processed = 0\n",
        "        with torch.no_grad():\n",
        "            # Use enumerate to get batch_idx if needed for logging\n",
        "            for batch_idx, batch_data in enumerate(train_loader_eval):\n",
        "                images, labels, label_lengths = batch_data\n",
        "                if images is None: continue\n",
        "\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                label_lengths = label_lengths.to(device)\n",
        "\n",
        "                # Check label lengths vs sequence lengths before calculating loss\n",
        "                sequence_length = images.size(3) // (2 * 2) # Assuming 2 conv+pool steps with kernel/stride 2\n",
        "                if torch.any(label_lengths <= 0) or torch.any(label_lengths > torch.full(size=(images.size(0),), fill_value=sequence_length, dtype=torch.int32).to(device)):\n",
        "                     # print(f\"Warning: Invalid label/sequence length in train eval epoch {i}, batch {batch_idx}. Skipping batch.\")\n",
        "                     continue\n",
        "\n",
        "                preds = model(images)\n",
        "                log_probs = F.log_softmax(preds, 2)\n",
        "                pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device)\n",
        "\n",
        "                try:\n",
        "                    loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "                    train_loss_epoch += loss.item()\n",
        "                    num_train_batches_processed += 1\n",
        "                except Exception as e:\n",
        "                     print(f\"Error calculating train eval loss in epoch {i}, batch {batch_idx}: {e}\")\n",
        "                     continue # Skip this batch\n",
        "\n",
        "            if num_train_batches_processed > 0:\n",
        "                train_loss_epoch /= num_train_batches_processed\n",
        "            else:\n",
        "                train_loss_epoch = float('inf') # Handle case where all batches are skipped\n",
        "                if len(train_loader_eval) > 0:\n",
        "                     print(f\"Warning: All batches skipped in train eval loader in epoch {i}. No train loss calculated.\")\n",
        "                else:\n",
        "                     print(f\"Warning: Train eval loader is empty in epoch {i}. No train loss calculated.\")\n",
        "\n",
        "        train_losses_hist.append(train_loss_epoch)\n",
        "\n",
        "\n",
        "        # Calculate average validation loss for this epoch\n",
        "        val_loss_epoch = 0\n",
        "        num_valid_batches_processed = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch_data in enumerate(val_loader_eval):\n",
        "                images, labels, label_lengths = batch_data\n",
        "                if images is None: continue\n",
        "\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                label_lengths = label_lengths.to(device)\n",
        "\n",
        "                # Check label lengths vs sequence lengths before calculating loss\n",
        "                sequence_length = images.size(3) // (2 * 2)\n",
        "                if torch.any(label_lengths <= 0) or torch.any(label_lengths > torch.full(size=(images.size(0),), fill_value=sequence_length, dtype=torch.int32).to(device)):\n",
        "                     # print(f\"Warning: Invalid label/sequence length in val eval epoch {i}, batch {batch_idx}. Skipping batch.\")\n",
        "                     continue\n",
        "\n",
        "                preds = model(images)\n",
        "                log_probs = F.log_softmax(preds, 2)\n",
        "                pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device)\n",
        "\n",
        "                try:\n",
        "                    loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "                    val_loss_epoch += loss.item()\n",
        "                    num_valid_batches_processed += 1\n",
        "                except Exception as e:\n",
        "                     print(f\"Error calculating val eval loss in epoch {i}, batch {batch_idx}: {e}\")\n",
        "                     continue # Skip this batch\n",
        "\n",
        "            if num_valid_batches_processed > 0:\n",
        "                val_loss_epoch /= num_valid_batches_processed\n",
        "            else:\n",
        "                val_loss_epoch = float('inf') # Handle case where all batches are skipped\n",
        "                if len(val_loader_eval) > 0:\n",
        "                     print(f\"Warning: All batches skipped in val eval loader in epoch {i}. No validation loss calculated.\")\n",
        "                else:\n",
        "                     print(f\"Warning: Validation eval loader is empty in epoch {i}. No validation loss calculated.\")\n",
        "\n",
        "        val_losses_hist.append(val_loss_epoch)\n",
        "\n",
        "        print(f\"Epoch {i}: Train Loss = {train_loss_epoch:.4f}, Validation Loss = {val_loss_epoch:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch potential errors during model loading or evaluation for a specific epoch\n",
        "        print(f\"Error processing epoch {i}: {e}\")\n",
        "\n",
        "\n",
        "  print(\"Finished generating graph data.\")\n",
        "\n",
        "  plt.figure()\n",
        "  plt.title(\"Loss Curve from Saved Models\")\n",
        "  plt.plot(epochs, train_losses_hist, label=\"Train Loss\")\n",
        "  plt.plot(epochs, val_losses_hist, label=\"Validation Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "# Example usage (Assuming train_CLSTM is called first to train and save models)\n",
        "# train_loader, val_loader, test_loader = get_data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "kOvvjOXD6q1o"
      ],
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}