{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcUPCkTny1Qw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e5de22-9570-4642-a4d4-dd2277b24509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "from torch.utils import data\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKID5n7myyhZ",
        "outputId": "b93062a3-ce30-4694-a5ee-d47b38da11a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6qHFoDy6Vda"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxgnY-y1vMp4",
        "outputId": "68671cf5-77b8-4240-e400-d2b5dfb9b25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How many images do you want to generate for training?15\n",
            "How many images do you want to generate for validation?15\n",
            "How many images do you want to generate for testing?15\n",
            "Operation completed. Time taken: 3.4401588439941406 seconds\n",
            "Operation completed. Time taken: 0.4662470817565918 seconds\n",
            "Operation completed. Time taken: 0.6050825119018555 seconds\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "from random import randint, choice, uniform\n",
        "import string\n",
        "import time\n",
        "\n",
        "fonts_drive_path = '/content/gdrive/My Drive/Miniproj/Fonts'\n",
        "colors = {\n",
        "  \"black\": 0x1c1c1c,\n",
        "  \"white\": 0xfcfcfc,\n",
        "}\n",
        "\n",
        "image_width = 250\n",
        "image_height = 70\n",
        "font_size = 50\n",
        "characters = [5,5]\n",
        "\n",
        "def get_text():\n",
        "  out_string = \"\"\n",
        "  for i in range(randint(characters[0], characters[1])):\n",
        "    out_string += choice(string.ascii_letters+\"0123456789\")\n",
        "  return out_string\n",
        "\n",
        "def draw_pixel(draw,x,y, thickness):\n",
        "  if(thickness > 1):\n",
        "    draw.line([(x, y), (x+thickness*([1,-1][randint(0, 1)]),y+thickness*([1,-1][randint(0, 1)]))], fill=colors[\"black\"], width=thickness)\n",
        "  else:\n",
        "    draw.line([(x, y), (x,y)], fill=colors[\"black\"])\n",
        "\n",
        "def add_noise(draw, amount, thickness):\n",
        "  for i in range(int(amount)):\n",
        "    draw_pixel(draw, randint(0, image_width), randint(0, image_height), thickness)\n",
        "\n",
        "def add_lines(draw, amount, thickness):\n",
        "\n",
        "  thickness = 1\n",
        "  wiggle_room_thickness = 2\n",
        "\n",
        "  for i in range(int(amount)):\n",
        "    wiggle_thickness = randint(thickness,thickness+wiggle_room_thickness)\n",
        "    draw.line([(randint(0, image_width), randint(0, image_height)), (randint(0, image_width),randint(0, image_height))], fill=colors[\"black\"],width=wiggle_thickness,)\n",
        "\n",
        "def draw_characters(draw, image, text):\n",
        "\n",
        "  fonts = [\"ComicSansMS3.ttf\",\"carbontype.ttf\",\"Kingthings_Trypewriter_2.ttf\",\"Sears_Tower.ttf\",\"TravelingTypewriter.ttf\"]\n",
        "  wiggle_room_width = 48 - len(text) * 6\n",
        "  wiggle_room_height = 13\n",
        "  width_padding = 30\n",
        "  font_size = 30\n",
        "  wiggle_room_font_size = 10\n",
        "  rotation_degrees = 20\n",
        "  wiggle_room_rotation_percent = 0.4\n",
        "\n",
        "  spacing_width = (image_width-width_padding) / len(text)\n",
        "  next_letter_pos = width_padding/2\n",
        "\n",
        "\n",
        "  for character in text:\n",
        "    wiggle_width = uniform(0,wiggle_room_width)\n",
        "    wiggle_height = uniform(0,wiggle_room_height)\n",
        "    wiggle_font_size = uniform(font_size - wiggle_room_font_size / 2, font_size  + wiggle_room_font_size / 2)\n",
        "    wiggle_rotation_degrees = rotation_degrees * uniform( -wiggle_room_rotation_percent,  wiggle_room_rotation_percent)\n",
        "    font = ImageFont.truetype(fonts_drive_path+\"/\"+fonts[randint(0,len(fonts)-1)], int(wiggle_font_size))\n",
        "    character_image = Image.new(\"RGB\", (image_height, image_height), 255)\n",
        "    draw = ImageDraw.Draw(character_image)\n",
        "    draw.rectangle([0, 0, image_height, image_height], fill=colors[\"white\"])\n",
        "    draw.text((wiggle_width, wiggle_height), character, fill=colors[\"black\"], font=font)\n",
        "    character_image = character_image.rotate(wiggle_rotation_degrees, expand=True, fillcolor=\"white\")\n",
        "    image.paste(character_image, (int(next_letter_pos), 0))\n",
        "    next_letter_pos += spacing_width\n",
        "\n",
        "def generate_data(num_samples, drive_path):\n",
        "  text_done = []\n",
        "  start = time.time()\n",
        "  for i in range(int(num_samples)):\n",
        "\n",
        "    image = Image.new('RGB',(image_width, image_height))\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    draw.rectangle([0, 0, image_width, image_height], fill=colors[\"white\"])\n",
        "\n",
        "    text = get_text()\n",
        "    if(text not in text_done):\n",
        "      text_done.append(text)\n",
        "    else:\n",
        "      print(\"Skipped\")\n",
        "      continue\n",
        "    draw_characters(draw, image, text)\n",
        "\n",
        "    add_noise(draw, amount=int(image_width*image_height*0.001), thickness = 1)\n",
        "    add_noise(draw, amount=int(image_width*image_height*0.001), thickness = 2)\n",
        "    add_noise(draw, amount=int(image_width*image_height*0.001), thickness = 3)\n",
        "    add_lines(draw, amount=1, thickness=2)\n",
        "    add_lines(draw, amount=1, thickness=3)\n",
        "\n",
        "\n",
        "    image.save(drive_path+\"/\"+text+\".jpg\")\n",
        "  end = time.time()\n",
        "  print(\"Operation completed. Time taken: \"+str(end - start)+\" seconds\")\n",
        "\n",
        "num_samples_train = input(\"How many images do you want to generate for training?\")\n",
        "drive_path_train = '/content/gdrive/My Drive/Miniproj/DS/Saved_Train'\n",
        "num_samples_validation = input(\"How many images do you want to generate for validation?\")\n",
        "drive_path_validation = '/content/gdrive/My Drive/Miniproj/DS/Saved_Validation'\n",
        "num_samples_testing = input(\"How many images do you want to generate for testing?\")\n",
        "drive_path_testing = '/content/gdrive/My Drive/Miniproj/DS/Saved_Test'\n",
        "generate_data(num_samples_train, drive_path_train)\n",
        "generate_data(num_samples_validation, drive_path_validation)\n",
        "generate_data(num_samples_testing, drive_path_testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2l_tlvZ72WI",
        "outputId": "c3b7c5df-bd3d-432d-fa79-52876671b4c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hpnipDU6cdh"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwyRgc7jMe6k"
      },
      "outputs": [],
      "source": [
        "#Enabling GPU Usage\n",
        "use_cuda = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qcWjNvWaBlKx",
        "outputId": "7760e914-c497-48e6-c2ec-48578c39473a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "all_characters = string.ascii_letters+\"0123456789\"\n",
        "all_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnUhEuBSB9yv"
      },
      "outputs": [],
      "source": [
        "def string_label(string):\n",
        "  out = []\n",
        "  index = 0\n",
        "  for char_string in string:\n",
        "    for char_index in range(0,len(all_characters)):\n",
        "      if char_string == all_characters[char_index]:\n",
        "        out.append(int(char_index))\n",
        "        break\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGRceeM9_I7v"
      },
      "outputs": [],
      "source": [
        "class CAPTCHAImageDataset(data.Dataset):\n",
        "  def __init__(self, img_dir, transform=None):\n",
        "    self.all_images = os.listdir(img_dir)\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.all_images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.all_images[idx]\n",
        "    # print(img_path)\n",
        "    label = img_path.split(\"/\")[-1]\n",
        "    label = img_path.split(\"-\")[0]\n",
        "    label = label.replace(\".jpg\",\"\")\n",
        "    # print(label)\n",
        "    image = Image.open(self.img_dir + \"/\" + img_path)\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    labels = torch.Tensor(string_label(label))\n",
        "    labels = labels.type(torch.LongTensor)\n",
        "    labels = labels.squeeze(0)\n",
        "    return image, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99MGbr-_6gc3"
      },
      "source": [
        "# Data Pre Processing\n",
        "Process the data by adding a filter, inverting colors, ensuring black and white"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vmJlZYqCgBI"
      },
      "outputs": [],
      "source": [
        "# Image pre-processing\n",
        "def remove_noise_and_grayscale(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  gray_median = cv2.medianBlur(gray_img, 3)\n",
        "  gray_thresh = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY_INV, cv2.THRESH_OTSU)[1]\n",
        "  return gray_thresh\n",
        "\n",
        "# cv2_imshow(cv2.imread('/content/gdrive/My Drive/Colab Notebooks/Datasets/project_dataset/Training/0By1b.jpg'))\n",
        "# gray_img = remove_noise_and_grayscale('/content/gdrive/My Drive/Colab Notebooks/Datasets/project_dataset/Training/0By1b.jpg')\n",
        "# cv2_imshow(gray_img)\n",
        "\n",
        "def generate_processed_data_folder(input_path, output_path):\n",
        "  # shutil.rmtree(output_path)\n",
        "  if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "    print(output_path)\n",
        "  for dir in os.listdir(input_path):\n",
        "    out_dir_path = os.path.join(output_path, dir)\n",
        "    if not os.path.exists(out_dir_path):\n",
        "      os.mkdir(out_dir_path)\n",
        "      print(out_dir_path)\n",
        "    in_dir_path = os.path.join(input_path, dir)\n",
        "    print(\"Processing images in: \" + dir)\n",
        "    for img in os.listdir(in_dir_path):\n",
        "      if os.path.isfile(os.path.join(in_dir_path, img)) and not os.path.isfile(os.path.join(out_dir_path, img)):\n",
        "        in_img_path = os.path.join(in_dir_path, img)\n",
        "        out_img_path = os.path.join(out_dir_path, img)\n",
        "        processed_img = remove_noise_and_grayscale(in_img_path)\n",
        "        print(in_img_path, out_img_path)\n",
        "        if not cv2.imwrite(out_img_path, processed_img):\n",
        "          raise Exception(\"Could not write image: {}\".format(out_img_path))\n",
        "  print(\"Done processing\")\n",
        "\n",
        "# def create_baseline_split_data(input_path, output_path):\n",
        "#   if not os.path.exists(output_path):\n",
        "#     os.mkdir(output_path)\n",
        "\n",
        "#   for dir in os.listdir(input_path):\n",
        "#     out_dir_path = os.path.join(output_path, dir)\n",
        "#     if not os.path.exists(output_path):\n",
        "#       os.mkdir(out_dir_path)\n",
        "#     in_dir_path = os.path.join(input_path, dir)\n",
        "#     print(\"Processing images in: \" + dir)\n",
        "#     image_count = 0\n",
        "#     for img in os.listdir(in_dir_path):\n",
        "#       image_count+=1\n",
        "#       if os.path.isfile(os.path.join(in_dir_path, img)):\n",
        "#         in_img_path = os.path.join(in_dir_path, img)\n",
        "#         transform = transforms.Compose([transforms.PILToTensor()])\n",
        "#         label = in_img_path.split(\"/\")[-1]\n",
        "#         label = label.replace(\".jpg\",\"\")\n",
        "#         current_label = 0\n",
        "#         img_pil = Image.open(in_img_path)\n",
        "#         num_chars = 5\n",
        "#         for index in range(0,num_chars):\n",
        "#           w,h = img_pil.size\n",
        "#           start_w = int(index * w/num_chars)\n",
        "#           end_w = int(min(w, (index+1) * w/num_chars))\n",
        "#           img_split = img_pil.crop((start_w,0,end_w,h))\n",
        "#           img_split.show()\n",
        "#           if not os.path.exists(out_dir_path+\"/\"+str(label[current_label])):\n",
        "#             os.mkdir(out_dir_path+\"/\"+str(label[current_label]))\n",
        "#           out_img_path = os.path.join(out_dir_path+\"/\"+str(label[current_label]), str(label[current_label])+\"-\"+str(image_count)+\".jpg\")\n",
        "#           img_split.save(out_img_path)\n",
        "#           current_label+=1\n",
        "#           image_count+=1\n",
        "#     print(\"Processed:\", image_count, \"character images\")\n",
        "#   print(\"Done processing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHB9UIzj0oF1"
      },
      "outputs": [],
      "source": [
        "def get_data_loader(batch_size, is_split_data=False):\n",
        "  transform = transforms.Compose([transforms.Resize((70,250)),transforms.ToTensor()])\n",
        "  if is_split_data==True:\n",
        "    transform = transforms.Compose([transforms.Resize((224,1120)),transforms.ToTensor()])\n",
        "  train_data = CAPTCHAImageDataset(drive_path + \"/Training\", transform=transform)\n",
        "  val_data = CAPTCHAImageDataset(drive_path + \"/Validation\", transform=transform)\n",
        "  test_data = CAPTCHAImageDataset(drive_path + \"/Testing\", transform=transform)\n",
        "\n",
        "  # print(train_data.__getitem__(1))\n",
        "  train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,num_workers=1,shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,num_workers=1,shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,num_workers=1,shuffle=True)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EXwEklI0rbi"
      },
      "outputs": [],
      "source": [
        "# Process data for the baseline model (estimate where the character is and split up the image accordingly)\n",
        "# Data splitting now happens when training the baseline model!\n",
        "# drive_path = '/content/gdrive/My Drive/Colab Notebooks/Datasets'\n",
        "# create_baseline_split_data(drive_path + '/project_dataset_processed', drive_path + '/baseline_split')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJyblq4y0sbM"
      },
      "outputs": [],
      "source": [
        "# drive_path = '/content/gdrive/My Drive/Colab Notebooks/Datasets/baseline_split'\n",
        "# train_loader, val_loader, test_loader = get_data_loader(1, is_split_data=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znk_-akhMwJf"
      },
      "outputs": [],
      "source": [
        "os.makedirs('/content/drive/MyDrive/Miniproj', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxzrK-xuNPNt",
        "outputId": "2076c799-9d0d-47f0-d904-24df09725071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing images in: Saved_Train\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/Ztcnr.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/Ztcnr.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/cqmqm.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/cqmqm.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/ZCUtd.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/ZCUtd.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/klfOm.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/klfOm.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/Bym8T.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/Bym8T.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/2wVpb.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/2wVpb.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/PiN2e.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/PiN2e.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/hzqLg.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/hzqLg.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/YpVwW.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/YpVwW.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/WwXsb.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/WwXsb.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/9ucao.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/9ucao.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/Jnz1k.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/Jnz1k.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/300qz.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/300qz.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/BPcHr.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/BPcHr.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Train/Wc1eR.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Train/Wc1eR.jpg\n",
            "Processing images in: Saved_Test\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/iC3mE.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/iC3mE.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/VASp7.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/VASp7.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/ZZzLg.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/ZZzLg.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/JIbS1.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/JIbS1.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/p9pjK.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/p9pjK.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/mlD5f.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/mlD5f.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/AiOVC.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/AiOVC.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/UCAUs.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/UCAUs.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/EIboE.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/EIboE.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/3TEnr.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/3TEnr.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/QXja7.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/QXja7.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/v8GM4.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/v8GM4.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/Om7OQ.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/Om7OQ.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/Mu1EI.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/Mu1EI.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Test/vWwpl.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Test/vWwpl.jpg\n",
            "Processing images in: Saved_Validation\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/a8rJQ.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/a8rJQ.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/NToys.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/NToys.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/QRYY3.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/QRYY3.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/RlX5W.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/RlX5W.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/CxqcO.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/CxqcO.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/gTtoy.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/gTtoy.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/4SGsQ.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/4SGsQ.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/zwF4Z.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/zwF4Z.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/pnblP.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/pnblP.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/EWh4P.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/EWh4P.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/uUNji.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/uUNji.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/zozMp.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/zozMp.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/caYzU.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/caYzU.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/PgyDo.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/PgyDo.jpg\n",
            "/content/drive/MyDrive/Miniproj/DS/Saved_Validation/YJ0yl.jpg /content/drive/MyDrive/Miniproj/DS/Processed/Saved_Validation/YJ0yl.jpg\n",
            "Processing images in: Processed\n",
            "Done processing\n"
          ]
        }
      ],
      "source": [
        "# Process the data by adding a filter, inverting colors, ensuring black and white\n",
        "drive_path = '/content/drive/MyDrive/Miniproj'\n",
        "#Corrected the output path to remove the extra drive path\n",
        "generate_processed_data_folder(os.path.join(drive_path, 'DS'), os.path.join(drive_path, 'DS/Processed'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iqYGzZjxJKi",
        "outputId": "97dfc663-0478-44bf-a69d-769770b9cb19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31\n",
            "31\n"
          ]
        }
      ],
      "source": [
        "drive_path = '/content/drive/MyDrive/Miniproj/DS'\n",
        "project_dataset_path = drive_path + '/content/drive/MyDrive/Miniproj/DS'\n",
        "project_dataset_p_path = drive_path + '/content/drive/MyDrive/Miniproj/DS/Processed'\n",
        "print(len(os.listdir(os.path.join(project_dataset_path, '/content/drive/MyDrive/Miniproj/DS/Saved_Validation'))))\n",
        "print(len(os.listdir(os.path.join(project_dataset_p_path, '/content/drive/MyDrive/Miniproj/DS/Saved_Validation'))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIV4wcg10mZX"
      },
      "outputs": [],
      "source": [
        "drive_path = '/content/drive/MyDrive/Miniproj/DS/Processed'\n",
        "train_loader, val_loader, test_loader = get_data_loader(168)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piX_iZQ46UOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f772f7-10b3-4d1b-b854-719d0ff36aaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "len(train_loader)\n",
        "#for images, labels in train_loader:\n",
        " # print(labels)\n",
        "  #print(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOgDfoyY6UyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9c27e8-e136-48bd-f0ae-f804e93b53a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "len(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoeTopaVznG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b557fde-c5d8-4c44-b280-a090977c7e8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POGH2DTj6V_Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "ea8c26c2-7052-4e84-96f4-9f41c1e980e5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_image(image):\n",
        "    image = torch.squeeze(image, dim=0)  # Remove batch dimension\n",
        "    img_np = image.numpy()\n",
        "\n",
        "    # Fix dimension issue before plotting\n",
        "    if img_np.shape[0] == 3:  # RGB Image (C, H, W)\n",
        "        plt.imshow(img_np.transpose(1, 2, 0))  # Convert to (H, W, C)\n",
        "    elif img_np.shape[0] == 1:  # Grayscale Image (1, H, W)\n",
        "        plt.imshow(img_np.squeeze(0), cmap='gray')  # Remove channel dimension\n",
        "    else:\n",
        "        plt.imshow(img_np)  # Already in (H, W, C) format\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "# Assuming `image` is a tensor with shape (C, H, W)\n",
        "image = torch.randn(1, 28, 28)  # Example grayscale image\n",
        "show_image(image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOvvjOXD6q1o"
      },
      "source": [
        "# Baseline CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1qvaC5kMaEA"
      },
      "outputs": [],
      "source": [
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "  \"\"\" Generate a name for the model consisting of all the hyperparameter values\"\"\"\n",
        "  path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,batch_size,learning_rate,epoch)\n",
        "  return path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycyxID93Mak1"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, data_loader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for imgs, labels in data_loader:\n",
        "    all_imgs_array = []\n",
        "    all_imgs = torch.Tensor()\n",
        "    for img in imgs:\n",
        "      split_imgs = split_image(img)\n",
        "      for split_img in split_imgs:\n",
        "        all_imgs_array.append(split_img)\n",
        "    all_imgs = torch.stack(all_imgs_array)\n",
        "\n",
        "    all_labels = torch.flatten(labels, start_dim=0, end_dim=1)\n",
        "\n",
        "    #To Enable GPU Usage\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "      all_imgs = all_imgs.cuda()\n",
        "      all_labels = all_labels.cuda()\n",
        "\n",
        "    output = model(all_imgs)\n",
        "    #select index with maximum prediction score\n",
        "    pred = output.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(all_labels.view_as(pred)).sum().item()\n",
        "    total += all_imgs.shape[0]\n",
        "    if total == 0:\n",
        "      return 0\n",
        "  return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSJq8bGxH0Fn"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cpu')\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88coIwsvMUo0"
      },
      "outputs": [],
      "source": [
        "class CNNCharNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNNCharNet, self).__init__()\n",
        "    self.name = \"CNNCharNet\"\n",
        "    # (224 - 5 + 2*0) / 1 + 1 = 220 x 220 (5 channels)\n",
        "    self.conv1 = nn.Conv2d(1,5,5)\n",
        "    # (220 - 2 + 2*0) / 2 + 1 = 110 x 110 (5 channels)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    # (110 - 5 + 2*0) / 1 + 1 = 106 x 106 (10 channels)\n",
        "    self.conv2 = nn.Conv2d(5,10,5)\n",
        "    # (106 - 2 + 2*0) / 2 + 1 = 53 x 53 (10 channels)\n",
        "    # self.fc1 = nn.Linear(53*53*10, 32)\n",
        "    # self.fc2 = nn.Linear(32,len(all_characters))\n",
        "    self.fc1 = nn.Linear(53*53*10, len(all_characters))\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1,53*53*10)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.softmax(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gYzgRO-eqgB"
      },
      "outputs": [],
      "source": [
        "def split_image(img, num_chars=5):\n",
        "  imgs = []\n",
        "  c, h, w = img.size()\n",
        "  for i in range(num_chars):\n",
        "    start_w = int(i * w/num_chars)\n",
        "    end_w = int(min(w, (i+1) * w/num_chars))\n",
        "    img_cropped = img[:, :, start_w:end_w]\n",
        "    imgs.append(img_cropped)\n",
        "    # print(img_cropped.size())\n",
        "  return imgs\n",
        "\n",
        "def train(model, train_loader, val_loader, test_laoder, batch_size=64, learning_rate=0.001, num_epochs=30):\n",
        "  # Check GPU availability\n",
        "  if use_cuda and torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "  else:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "\n",
        "  # Fixed PyTorch random seed for reproducible result\n",
        "  torch.manual_seed(1000)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  iters, losses, train_acc, val_acc = [], [], [], []\n",
        "\n",
        "  print(\"Training...\")\n",
        "  start_time = time.time()\n",
        "\n",
        "  # training\n",
        "  n = 0 # the number of iterations\n",
        "  # Set up some numpy arrays to store the accuracy\n",
        "  for epoch in range(num_epochs):\n",
        "    for imgs, labels in iter(train_loader):\n",
        "      # print(labels)\n",
        "      all_imgs_array = []\n",
        "      all_imgs = torch.Tensor()\n",
        "      # print(imgs.size())\n",
        "      for img in imgs:\n",
        "        split_imgs = split_image(img)\n",
        "        for split_img in split_imgs:\n",
        "          all_imgs_array.append(split_img)\n",
        "      all_imgs = torch.stack(all_imgs_array)\n",
        "\n",
        "      all_labels = torch.flatten(labels, start_dim=0, end_dim=1)\n",
        "\n",
        "      #To Enable GPU Usage\n",
        "      if use_cuda and torch.cuda.is_available():\n",
        "        all_imgs = all_imgs.cuda()\n",
        "        all_labels = all_labels.cuda()\n",
        "      out = model(all_imgs)         # forward pass\n",
        "      loss = criterion(out, all_labels) # compute the total loss\n",
        "      loss.backward()               # backward pass (compute parameter updates)\n",
        "      optimizer.step()              # make the updates for each parameter\n",
        "      optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "    # save the current training information\n",
        "    iters.append(n)\n",
        "    losses.append(float(loss)/batch_size)             # compute *average* loss\n",
        "    train_acc.append(get_accuracy(model, train_loader)) # compute training accuracy\n",
        "    val_acc.append(get_accuracy(model, val_loader))  # compute validation accuracy\n",
        "    n += 1\n",
        "\n",
        "    print((\"Epoch {}: Train accuracy: {} | Validation accuracy: {}\").format(epoch + 1,train_acc[epoch],val_acc[epoch]))\n",
        "\n",
        "    # Save the current model (checkpoint) to a file\n",
        "    model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "  print('Finished Training')\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "\n",
        "  # plotting\n",
        "  import matplotlib.pyplot as plt\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters, losses, label=\"Train\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters, train_acc, label=\"Train\")\n",
        "  plt.plot(iters, val_acc, label=\"Validation\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Training Accuracy\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "  print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC6DJW13z0KP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e3490d63-53d6-44ed-8854-74425acf7acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available.  Training on CPU ...\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-25-3607764350.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = F.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train accuracy: 0.48 | Validation accuracy: 0.0\n",
            "Epoch 2: Train accuracy: 0.52 | Validation accuracy: 0.0\n",
            "Epoch 3: Train accuracy: 0.56 | Validation accuracy: 0.0\n",
            "Epoch 4: Train accuracy: 0.56 | Validation accuracy: 0.0\n",
            "Epoch 5: Train accuracy: 0.56 | Validation accuracy: 0.0\n",
            "Epoch 6: Train accuracy: 0.56 | Validation accuracy: 0.0\n",
            "Epoch 7: Train accuracy: 0.56 | Validation accuracy: 0.0\n",
            "Epoch 8: Train accuracy: 0.56 | Validation accuracy: 0.0\n",
            "Epoch 9: Train accuracy: 0.56 | Validation accuracy: 0.0\n",
            "Epoch 10: Train accuracy: 0.56 | Validation accuracy: 0.0125\n",
            "Epoch 11: Train accuracy: 0.56 | Validation accuracy: 0.0125\n",
            "Epoch 12: Train accuracy: 0.56 | Validation accuracy: 0.0125\n",
            "Epoch 13: Train accuracy: 0.6 | Validation accuracy: 0.0125\n",
            "Epoch 14: Train accuracy: 0.6 | Validation accuracy: 0.0\n",
            "Epoch 15: Train accuracy: 0.6 | Validation accuracy: 0.0\n",
            "Epoch 16: Train accuracy: 0.6 | Validation accuracy: 0.0\n",
            "Epoch 17: Train accuracy: 0.6 | Validation accuracy: 0.0\n",
            "Epoch 18: Train accuracy: 0.6 | Validation accuracy: 0.0\n",
            "Epoch 19: Train accuracy: 0.6 | Validation accuracy: 0.0\n",
            "Epoch 20: Train accuracy: 0.6 | Validation accuracy: 0.0\n",
            "Finished Training\n",
            "Total time elapsed: 317.47 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Accuracy: 0.6\n",
            "Final Validation Accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Process data for the baseline model (estimate where the character is and split up the image accordingly)\n",
        "drive_path = '/content/drive/MyDrive/Miniproj/DS/Processed'\n",
        "train_loader, val_loader, test_loader = get_data_loader(64, is_split_data=True)\n",
        "model = CNNCharNet()\n",
        "train(model, train_loader, val_loader, test_loader, batch_size=64, learning_rate=0.001, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVdqah5vR6Hv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c184ada2-5e00-4261-b7df-5bc1ef562913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_CNNCharNet_bs64_lr0.001_epoch19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-25-3607764350.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = F.softmax(x)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Get the accuracy of the baseline\n",
        "model_path = get_model_name(model.name, batch_size=64, learning_rate=0.001, epoch=20-1)\n",
        "print(model_path)\n",
        "state = torch.load(model_path)\n",
        "model.load_state_dict(state)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "get_accuracy(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSD_9LvL6vnB"
      },
      "source": [
        "# Primary Model\n",
        "Bidrectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw6l17sPYx3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "780cef40-3245-4fcc-a964-6685288824fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0039, 0.0039]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0039, 0.0039, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])\n",
            "tensor([[12,  2, 20, 45, 18],\n",
            "        [50, 43, 30,  5, 59],\n",
            "        [ 3, 15,  4, 27, 19],\n",
            "        [14, 61, 43, 22, 43],\n",
            "        [ 3, 48, 14, 42, 41]])\n"
          ]
        }
      ],
      "source": [
        "for img, labels in test_loader:\n",
        "  print(img)\n",
        "  print(labels)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MgpaZp_tygwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why are we using this loop:\n",
        "\n",
        "The for loop retrieves and prints a single batch of test images and their corresponding labels from the DataLoader to verify the data structure before model evaluation.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "To verify image data shape\n",
        "**bold text** * Are the images of shape [batch_size, 1, 28, 28]? (for grayscale)\n",
        "\n",
        "To inspect labels\n",
        "  * Are the labels aligned correctly?\n",
        "  * Do they contain 5 integers per image (indicating 5 characters)?\n",
        "\n",
        "Debugging\n",
        "  * Check if preprocessing worked (e.g., normalization, resizing)\n",
        "  * Confirm whether images and labels are paired correctly\n",
        "\n",
        "\n",
        "   Useful before training or evaluating your model"
      ],
      "metadata": {
        "id": "sGQl4rrXyGqG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjbAOsNqqS64"
      },
      "outputs": [],
      "source": [
        "# # Bidirectional recurrent neural network\n",
        "# class BiRNN(nn.Module):\n",
        "#     def __init__(self, input_size, n_hidden, n_layers):\n",
        "#         super(BiRNN, self).__init__()\n",
        "#         self.name = 'BiRNN'\n",
        "#         self.input_size = input_size\n",
        "#         self.n_hidden = n_hidden\n",
        "#         self.n_layers = n_layers\n",
        "#         self.lstm = nn.LSTM(input_size, n_hidden, n_layers, batch_first=True, bidirectional=True)\n",
        "#         self.fc = nn.Linear(n_hidden*2, 1) #2 for bidirectional\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Set initial states\n",
        "#         h_0 = torch.zeros(self.n_layers*2, x.size(0), self.n_hidden)\n",
        "#         c_0 = torch.zeros(self.n_layers*2, x.size(0), self.n_hidden)\n",
        "\n",
        "#         # Forward propagate LSTM\n",
        "#         output, hidden = self.lstm(x, (h_0, c_0))\n",
        "\n",
        "#         # Decode the hidden state of the last time step\n",
        "#         output = self.fc(output[:, -1, :])\n",
        "#         output = F.softmax(output)\n",
        "#         return output\n",
        "\n",
        "# model = BiRNN(input_size = len(all_characters),n_hidden = 10,n_layers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDiswu7YFZMX"
      },
      "outputs": [],
      "source": [
        "# class CAPTCHANet(nn.Module):\n",
        "#   def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "#     super(CAPTCHANet, self).__init__()\n",
        "#     self.name = \"CAPTCHANet\"\n",
        "#     self.emb = torch.eye(input_size)\n",
        "#     self.hidden_size = hidden_size\n",
        "#     self.num_layers = num_layers\n",
        "#     self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional = True)\n",
        "#     self.fc = nn.Linear(hidden_size * 2, num_classes) # multiply by 2 -> one forward, one backward\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # multiply by 2 -> one forward, one backward\n",
        "#     c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # multiply by 2 -> one forward, one backward\n",
        "\n",
        "#     print(x[0][0].size(0))\n",
        "#     print(len(all_characters))\n",
        "#     x = self.emb(x)\n",
        "#     print(\"OK\")\n",
        "#     out, _ = self.rnn(x, (h0, c0))\n",
        "#     out = torch.cat([torch.max(out, dim=1)[0], torch.mean(out, dim=1)], dim=1)\n",
        "#     return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0lu6FsAQxKA"
      },
      "outputs": [],
      "source": [
        "# model = CAPTCHANet(len(all_characters), hidden_size = 10, num_layers=2, num_classes = 1)\n",
        "# train_loader, val_loader, test_loader = get_data_loader(1)\n",
        "# train(model, train_loader, val_loader, num_epochs=5, learning_rate=0.001, batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoMv5ygNG-7_"
      },
      "outputs": [],
      "source": [
        "# def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-3, batch_size=256):\n",
        "#   # Check GPU availability\n",
        "#   if use_cuda and torch.cuda.is_available():\n",
        "#     model.cuda()\n",
        "#     print('CUDA is available!  Training on GPU ...')\n",
        "#   else:\n",
        "#     print('CUDA is not available.  Training on CPU ...')\n",
        "\n",
        "#   start_time = time.time()\n",
        "\n",
        "#   torch.manual_seed(42)\n",
        "#   criterion = nn.CrossEntropyLoss()\n",
        "#   optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#   train_acc, valid_acc, losses, iters = [], [], [], []\n",
        "#   n = 0 # the number of iterations\n",
        "\n",
        "#   for epoch in range(num_epochs):\n",
        "#     total_train_loss = 0.0\n",
        "#     for imgs, labels in iter(train_loader):\n",
        "#       if use_cuda and torch.cuda.is_available():\n",
        "#         imgs = imgs.cuda()\n",
        "#         labels = labels.cuda()\n",
        "#       pred = model(imgs)            # forward pass\n",
        "#       loss = criterion(pred, labels)# compute the total loss\n",
        "#       loss.backward()               # backward pass (compute parameter updates)\n",
        "#       optimizer.step()              # make the updates for each parameter\n",
        "#       optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "#     # save the current training information\n",
        "#     iters.append(n)\n",
        "#     losses.append(float(loss)/batch_size)               # compute *average* loss\n",
        "#     train_acc.append(get_accuracy(model, train_loader)) # compute training accuracy\n",
        "#     valid_acc.append(get_accuracy(model, val_loader))   # compute validation accuracy\n",
        "#     n += 1\n",
        "\n",
        "\n",
        "#     print((\"Epoch {}: Train accuracy: {} | Validation accuracy: {}\").format(epoch + 1,train_acc[epoch],val_acc[epoch]))\n",
        "\n",
        "#     # Save the current model (checkpoint) to a file\n",
        "#     model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n",
        "#     torch.save(model.state_dict(), model_path)\n",
        "\n",
        "#     print('Finished Training')\n",
        "#     end_time = time.time()\n",
        "#     elapsed_time = end_time - start_time\n",
        "#     print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "\n",
        "#     # plotting\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     plt.title(\"Training Curve\")\n",
        "#     plt.plot(iters, losses, label=\"Train\")\n",
        "#     plt.xlabel(\"Iterations\")\n",
        "#     plt.ylabel(\"Loss\")\n",
        "#     plt.show()\n",
        "\n",
        "#     plt.title(\"Training Curve\")\n",
        "#     plt.plot(iters, train_acc, label=\"Train\")\n",
        "#     plt.plot(iters, valid_acc, label=\"Validation\")\n",
        "#     plt.xlabel(\"Iterations\")\n",
        "#     plt.ylabel(\"Training Accuracy\")\n",
        "#     plt.legend(loc='best')\n",
        "#     plt.show()\n",
        "\n",
        "#     print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "#     print(\"Final Validation Accuracy: {}\".format(valid_acc[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fcFvUyjaIRP"
      },
      "outputs": [],
      "source": [
        "class CLSTM(nn.Module):\n",
        "  def __init__(self, input_size=256, hidden_size=128, num_class=62):\n",
        "    super(CLSTM, self).__init__()\n",
        "    self.name = \"CLSTM\"\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "    # Calculate the input size for fc1 dynamically\n",
        "    self.fc1 = nn.Linear(16 * 128, input_size) #Changed to calculate dynamically\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.rnn = nn.LSTM(input_size=256, hidden_size=128, bidirectional=True, num_layers=2, batch_first=True)\n",
        "    self.fc2 = nn.Linear(hidden_size*2, num_class+1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    # Calculate the input size for fc1 dynamically\n",
        "    x = x.view(batch_size, x.size(1), -1) #Changed to calculate dynamically\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc2(x)\n",
        "    # Put in required shape for to get CTC loss\n",
        "    x = x.permute(1, 0, 2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC1cMjDK5aed"
      },
      "outputs": [],
      "source": [
        "def train_CLSTM(model, train_loader, val_loader, batch_size=128, num_epochs=400, lr=1e-4):\n",
        "  drive_path = '/content/drive/MyDrive/Miniproj/Datasets/project_dataset_processed'\n",
        "  train_loader_single, val_loader_single, test_loader_single = get_data_loader(1)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  criterion = nn.CTCLoss(blank=0)\n",
        "  iters_loss, iters_acc, train_losses, valid_losses, train_accs, val_accs = [], [], [], [], [], []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      labels = labels + 1 #account for unknown\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      preds = model(images)\n",
        "      log_probs = F.log_softmax(preds, 2)\n",
        "      pred_lengths = torch.full(size=(preds.size(1),), fill_value=log_probs.size(0), dtype=torch.int32)\n",
        "      label_lengths = torch.full(size=(preds.size(1),), fill_value=labels.size(1), dtype=torch.int32)\n",
        "      loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "\n",
        "    iters_loss.append(epoch)\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    valid_loss = 0\n",
        "    for images, labels in val_loader:\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      labels = labels + 1 #account for unknown\n",
        "\n",
        "      preds = model(images)\n",
        "      log_probs = F.log_softmax(preds, 2)\n",
        "      pred_lengths = torch.full(size=(preds.size(1),), fill_value=log_probs.size(0), dtype=torch.int32)\n",
        "      label_lengths = torch.full(size=(preds.size(1),), fill_value=labels.size(1), dtype=torch.int32)\n",
        "      loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    valid_loss /= len(val_loader)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(\"Epoch: {} Train Loss: {} Valid Loss: {}\".format(epoch, train_loss, valid_loss))\n",
        "    if ((epoch + 1) % 10 == 0):\n",
        "      if epoch != 99:\n",
        "        model_path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(model.name,128,1e-4,epoch + 1)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(\"saving\")\n",
        "\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters_loss, train_losses, label=\"Train\")\n",
        "  plt.plot(iters_loss, valid_losses, label=\"Validation\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEQvEYnV9FIh"
      },
      "outputs": [],
      "source": [
        "class CLSTM(nn.Module):\n",
        "  def __init__(self, input_size=256, hidden_size=128, num_class=62):\n",
        "    super(CLSTM, self).__init__()\n",
        "    self.name = \"CLSTM\"\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "    # Remove the hardcoded input size for fc1\n",
        "    # self.fc1 = nn.Linear(16 * 128, input_size)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.rnn = nn.LSTM(input_size=256, hidden_size=128, bidirectional=True, num_layers=2, batch_first=True)\n",
        "    self.fc2 = nn.Linear(hidden_size*2, num_class+1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    # Calculate the input size for fc1 dynamically\n",
        "    x = x.view(batch_size, x.size(1), -1)\n",
        "    # Dynamically determine the input size and initialize fc1\n",
        "    input_feature_size = x.shape[2]  # Get the feature size from the reshaped tensor\n",
        "    self.fc1 = nn.Linear(input_feature_size, 256)  # Initialize fc1 with the correct input size\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc2(x)\n",
        "    # Put in required shape for to get CTC loss\n",
        "    x = x.permute(1, 0, 2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CQauYUJA-yv"
      },
      "outputs": [],
      "source": [
        "class CLSTM(nn.Module):\n",
        "  def __init__(self, input_size=256, hidden_size=128, num_class=62):\n",
        "    super(CLSTM, self).__init__()\n",
        "    self.name = \"CLSTM\"\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "\n",
        "    # Dynamically calculate the input size for the first fully connected layer (self.fc1)\n",
        "    # This is done by passing a dummy tensor through the convolutional and pooling layers\n",
        "    # Assume input image size is 70x250 (from the get_data_loader transform)\n",
        "    dummy_input = torch.randn(1, 1, 70, 250)\n",
        "    x = F.relu(self.conv1(dummy_input))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.view(x.size(0), x.size(1), -1)\n",
        "    input_feature_size = x.shape[2]\n",
        "\n",
        "    self.fc1 = nn.Linear(input_feature_size, input_size)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=True, num_layers=2, batch_first=True)\n",
        "    self.fc2 = nn.Linear(hidden_size*2, num_class+1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.view(batch_size, x.size(1), -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc2(x)\n",
        "    # Put in required shape for to get CTC loss\n",
        "    x = x.permute(1, 0, 2)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8UX2Vl0DA4n3",
        "outputId": "d06e00d8-6666-44ee-ac58-e13894b0b3e2",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Generating graph data from saved models...\n",
            "Warning: Model file not found for epoch 10 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch10. Skipping.\n",
            "Warning: Model file not found for epoch 20 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch20. Skipping.\n",
            "Warning: Model file not found for epoch 30 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch30. Skipping.\n",
            "Warning: Model file not found for epoch 40 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch40. Skipping.\n",
            "Warning: Model file not found for epoch 50 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch50. Skipping.\n",
            "Warning: Model file not found for epoch 60 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch60. Skipping.\n",
            "Warning: Model file not found for epoch 70 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch70. Skipping.\n",
            "Warning: Model file not found for epoch 80 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch80. Skipping.\n",
            "Warning: Model file not found for epoch 90 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch90. Skipping.\n",
            "Warning: Model file not found for epoch 100 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch100. Skipping.\n",
            "Warning: Model file not found for epoch 110 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch110. Skipping.\n",
            "Warning: Model file not found for epoch 120 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch120. Skipping.\n",
            "Warning: Model file not found for epoch 130 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch130. Skipping.\n",
            "Warning: Model file not found for epoch 140 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch140. Skipping.\n",
            "Warning: Model file not found for epoch 150 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch150. Skipping.\n",
            "Warning: Model file not found for epoch 160 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch160. Skipping.\n",
            "Warning: Model file not found for epoch 170 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch170. Skipping.\n",
            "Warning: Model file not found for epoch 180 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch180. Skipping.\n",
            "Warning: Model file not found for epoch 190 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch190. Skipping.\n",
            "Warning: Model file not found for epoch 200 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch200. Skipping.\n",
            "Warning: Model file not found for epoch 210 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch210. Skipping.\n",
            "Warning: Model file not found for epoch 220 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch220. Skipping.\n",
            "Warning: Model file not found for epoch 230 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch230. Skipping.\n",
            "Warning: Model file not found for epoch 240 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch240. Skipping.\n",
            "Warning: Model file not found for epoch 250 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch250. Skipping.\n",
            "Warning: Model file not found for epoch 260 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch260. Skipping.\n",
            "Warning: Model file not found for epoch 270 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch270. Skipping.\n",
            "Warning: Model file not found for epoch 280 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch280. Skipping.\n",
            "Warning: Model file not found for epoch 290 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch290. Skipping.\n",
            "Warning: Model file not found for epoch 300 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch300. Skipping.\n",
            "Warning: Model file not found for epoch 310 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch310. Skipping.\n",
            "Warning: Model file not found for epoch 320 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch320. Skipping.\n",
            "Warning: Model file not found for epoch 330 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch330. Skipping.\n",
            "Warning: Model file not found for epoch 340 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch340. Skipping.\n",
            "Warning: Model file not found for epoch 350 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch350. Skipping.\n",
            "Warning: Model file not found for epoch 360 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch360. Skipping.\n",
            "Warning: Model file not found for epoch 370 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch370. Skipping.\n",
            "Warning: Model file not found for epoch 380 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch380. Skipping.\n",
            "Warning: Model file not found for epoch 390 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch390. Skipping.\n",
            "Warning: Model file not found for epoch 400 at /content/gdrive/My Drive/Miniproj/Models/model_CLSTM_bs128_lr0.0001_epoch400. Skipping.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "from torch.utils import data\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow # Commented out as it's not used and might cause issues\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Ensure Google Drive is mounted if it wasn't already\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Ensure all_characters is defined as it's used in decode_label and decode_pred\n",
        "all_characters = string.ascii_letters + string.digits\n",
        "\n",
        "# Define the directory for saving models within your Google Drive\n",
        "# Adjust this path if you prefer a different location\n",
        "model_save_dir = '/content/gdrive/My Drive/Miniproj/Models'\n",
        "\n",
        "# Create the model save directory if it doesn't exist\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "  \"\"\" Generate a name for the model including the save directory \"\"\"\n",
        "  # Construct the filename\n",
        "  filename = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,batch_size,learning_rate,epoch)\n",
        "  # Combine the save directory path with the filename\n",
        "  path = os.path.join(model_save_dir, filename)\n",
        "  return path\n",
        "\n",
        "# Assuming CLSTM class is defined in a previous cell and is correct.\n",
        "# Including the definition here for completeness based on ipython-input-132\n",
        "class CLSTM(nn.Module):\n",
        "  def __init__(self, input_size=256, hidden_size=128, num_class=62):\n",
        "    super(CLSTM, self).__init__()\n",
        "    self.name = \"CLSTM\"\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "\n",
        "    # Dynamically calculate the input size for the first fully connected layer (self.fc1)\n",
        "    # This is done by passing a dummy tensor through the convolutional and pooling layers\n",
        "    # Assume input image size is 70x250 (from the get_data_loader transform)\n",
        "    dummy_input = torch.randn(1, 1, 70, 250)\n",
        "    x = F.relu(self.conv1(dummy_input))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.view(x.size(0), x.size(1), -1)\n",
        "    input_feature_size = x.shape[2]\n",
        "\n",
        "    self.fc1 = nn.Linear(input_feature_size, input_size)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=True, num_layers=2, batch_first=True)\n",
        "    self.fc2 = nn.Linear(hidden_size*2, num_class+1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.view(batch_size, x.size(1), -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc2(x)\n",
        "    # Put in required shape for to get CTC loss\n",
        "    x = x.permute(1, 0, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "# Assuming get_data_loader and decode_label functions are defined in previous cells and are correct.\n",
        "# Including placeholder definitions if needed, but ideally they should be run before this.\n",
        "def string_label(string):\n",
        "  out = []\n",
        "  index = 0\n",
        "  for char_string in string:\n",
        "    for char_index in range(0,len(all_characters)):\n",
        "      if char_string == all_characters[char_index]:\n",
        "        out.append(int(char_index))\n",
        "        break\n",
        "  return out\n",
        "\n",
        "class CAPTCHAImageDataset(data.Dataset):\n",
        "  def __init__(self, img_dir, transform=None):\n",
        "    if not os.path.exists(img_dir):\n",
        "        # Corrected the error message to show the directory path\n",
        "        raise FileNotFoundError(f\"Image directory not found: {img_dir}\")\n",
        "    self.all_images = os.listdir(img_dir)\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.all_images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if idx >= len(self.all_images):\n",
        "         raise IndexError(f\"Index {idx} out of bounds for dataset of size {len(self)}\")\n",
        "\n",
        "    img_path_relative = self.all_images[idx]\n",
        "    full_img_path = os.path.join(self.img_dir, img_path_relative)\n",
        "\n",
        "    label_filename = img_path_relative.split(\".\")[0]\n",
        "    # Corrected label extraction - assuming filename format is like 'Label-OptionalStuff.jpg' or just 'Label.jpg'\n",
        "    # This line was label = img_path.split(\"/\")[-1] -> label = img_path.split(\"-\")[0] -> label = label.replace(\".jpg\",\"\")\n",
        "    # Let's simplify to just get the part before the first hyphen or the extension.\n",
        "    parts = label_filename.split(\"-\")\n",
        "    label = parts[0]\n",
        "\n",
        "    # Corrected image loading to handle potential issues and ensure grayscale for single channel model\n",
        "    try:\n",
        "        image = Image.open(full_img_path).convert(\"L\") # Convert to grayscale\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error opening image file: {full_img_path}\")\n",
        "        # Return None or handle the error appropriately\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing image {full_img_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "\n",
        "    # Ensure labels are processed correctly for CTC Loss\n",
        "    labels_list = string_label(label)\n",
        "    if not labels_list:\n",
        "        # Handle cases where string_label returns an empty list (e.g., label has no valid characters)\n",
        "        print(f\"Warning: Could not generate labels for filename {img_path_relative}. Skipping.\")\n",
        "        return None, None\n",
        "\n",
        "    # CTC Loss typically expects a sequence of integers\n",
        "    labels = torch.Tensor(labels_list).type(torch.LongTensor)\n",
        "\n",
        "    return image, labels\n",
        "\n",
        "def get_data_loader(batch_size, is_split_data=False):\n",
        "  # Use the correct base path for the processed data\n",
        "  drive_path_dataset = '/content/gdrive/My Drive/Miniproj/DS/Processed'\n",
        "\n",
        "  transform = transforms.Compose([transforms.Resize((70,250)),transforms.ToTensor()])\n",
        "\n",
        "  # Note: is_split_data=True is intended for the Baseline CNN and uses different dimensions/data structure.\n",
        "  # This CLSTM model expects the full 70x250 image.\n",
        "  if is_split_data:\n",
        "      print(\"Warning: get_data_loader called with is_split_data=True for CLSTM model. This might not be intended.\")\n",
        "      # You might want to raise an error or adjust transforms/dataset accordingly\n",
        "      # Keeping the original transform for the full image for the CLSTM\n",
        "      pass\n",
        "\n",
        "\n",
        "  # Corrected paths to point to the subdirectories within the processed data folder\n",
        "  train_data = CAPTCHAImageDataset(os.path.join(drive_path_dataset, \"Saved_Train\"), transform=transform)\n",
        "  val_data = CAPTCHAImageDataset(os.path.join(drive_path_dataset, \"Saved_Validation\"), transform=transform)\n",
        "  test_data = CAPTCHAImageDataset(os.path.join(drive_path_dataset, \"Saved_Test\"), transform=transform)\n",
        "\n",
        "  # Custom collate function to handle variable sequence lengths for CTC loss\n",
        "  # This function filters out None items and handles padding for labels if necessary.\n",
        "  # For CTC loss, labels are typically padded, not images/features directly in the DataLoader.\n",
        "  # The CLSTM forward pass handles padding for the output sequence implicitly via the permutation.\n",
        "  # We just need to make sure the labels tensor is created correctly.\n",
        "  def collate_fn(batch):\n",
        "      # Filter out samples where loading/processing failed\n",
        "      batch = [item for item in batch if item[0] is not None and item[1] is not None]\n",
        "      if not batch:\n",
        "          return None, None # Return None if the batch is empty after filtering\n",
        "\n",
        "      images, labels = zip(*batch)\n",
        "\n",
        "      # Stack images (they should already be tensors from the transform)\n",
        "      images = torch.stack(images, 0)\n",
        "\n",
        "      # Labels need to be concatenated and provided with lengths for CTCLoss\n",
        "      label_lengths = torch.LongTensor([len(label) for label in labels])\n",
        "      labels = torch.cat(labels, 0)\n",
        "\n",
        "      return images, labels, label_lengths\n",
        "\n",
        "  # Use the custom collate function\n",
        "  train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
        "  val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "  return train_loader, val_loader, test_loader\n",
        "\n",
        "# Updated train_CLSTM function to save models in the specified directory\n",
        "def train_CLSTM(model, train_loader, val_loader, batch_size=128, num_epochs=400, lr=1e-4):\n",
        "  # Removed the single image loaders here as they are not needed for training loop\n",
        "  # drive_path = '/content/drive/MyDrive/Miniproj/Datasets/project_dataset_processed'\n",
        "  # train_loader_single, val_loader_single, test_loader_single = get_data_loader(1)\n",
        "\n",
        "  # Check GPU availability and move model\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "  print(f\"Training on: {device}\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  # CTCLoss expects blank=0 by default. Our labels were adjusted by +1, so blank should be 0.\n",
        "  criterion = nn.CTCLoss(blank=0)\n",
        "  # Removed iters_acc, train_accs, val_accs as accuracy calculation is complex for CTC\n",
        "  iters_loss, train_losses, valid_losses = [], [], []\n",
        "\n",
        "  print(\"Starting CLSTM Training...\")\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    train_loss = 0\n",
        "    # Updated data loading to match collate_fn output\n",
        "    for batch_idx, (images, labels, label_lengths) in enumerate(train_loader):\n",
        "      if images is None:\n",
        "          # Skip if the batch was empty after filtering\n",
        "          print(f\"Warning: Empty batch at epoch {epoch}, batch {batch_idx}. Skipping.\")\n",
        "          continue\n",
        "\n",
        "      images = images.to(device)\n",
        "      # Labels and label_lengths are needed by CTCLoss on the same device\n",
        "      labels = labels.to(device)\n",
        "      label_lengths = label_lengths.to(device)\n",
        "\n",
        "      # Labels are already adjusted by +1 in the collate_fn or get_item\n",
        "      # Removed the labels = labels + 1 line here\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      preds = model(images)\n",
        "      log_probs = F.log_softmax(preds, 2) # Shape: (sequence_length, batch_size, num_classes)\n",
        "      # pred_lengths should be the length of the sequence dimension for each item in the batch\n",
        "      pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device) # Ensure on device\n",
        "\n",
        "      # Check for potential issues with label_lengths being zero or exceeding sequence_length\n",
        "      if torch.any(label_lengths <= 0):\n",
        "          print(f\"Warning: Zero or negative label length detected in epoch {epoch}, batch {batch_idx}. Skipping batch.\")\n",
        "          continue\n",
        "      if torch.any(label_lengths > pred_lengths):\n",
        "          print(f\"Warning: Label length exceeds sequence length in epoch {epoch}, batch {batch_idx}. Skipping batch.\")\n",
        "          continue\n",
        "\n",
        "      try:\n",
        "          loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_loss += loss.item()\n",
        "      except Exception as e:\n",
        "          print(f\"Error during loss calculation/backward pass in epoch {epoch}, batch {batch_idx}: {e}\")\n",
        "          # Optionally, log more details or skip the batch\n",
        "          continue\n",
        "\n",
        "\n",
        "    iters_loss.append(epoch)\n",
        "    # Ensure train_loss is not divided by zero if train_loader is empty\n",
        "    if len(train_loader) > 0:\n",
        "        train_loss /= len(train_loader)\n",
        "    else:\n",
        "        train_loss = float('inf') # Or 0, depending on desired behavior for empty loader\n",
        "        print(f\"Warning: Train loader is empty in epoch {epoch}.\")\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    valid_loss = 0\n",
        "    with torch.no_grad(): # Disable gradient calculation for validation\n",
        "        # Updated data loading to match collate_fn output\n",
        "        for images, labels, label_lengths in val_loader:\n",
        "          if images is None:\n",
        "              print(f\"Warning: Empty validation batch at epoch {epoch}, batch {batch_idx}. Skipping.\")\n",
        "              continue\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          label_lengths = label_lengths.to(device)\n",
        "\n",
        "          # Labels are already adjusted by +1 in the collate_fn or get_item\n",
        "          # Removed the labels = labels + 1 line here\n",
        "\n",
        "          preds = model(images)\n",
        "          log_probs = F.log_softmax(preds, 2)\n",
        "          pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device)\n",
        "\n",
        "          if torch.any(label_lengths <= 0) or torch.any(label_lengths > pred_lengths):\n",
        "              print(f\"Warning: Invalid label/sequence length in validation epoch {epoch}, batch {batch_idx}. Skipping batch.\")\n",
        "              continue\n",
        "\n",
        "          try:\n",
        "              loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "              valid_loss += loss.item()\n",
        "          except Exception as e:\n",
        "              print(f\"Error during validation loss calculation in epoch {epoch}, batch {batch_idx}: {e}\")\n",
        "              continue\n",
        "\n",
        "    # Ensure valid_loss is not divided by zero if val_loader is empty\n",
        "    if len(val_loader) > 0:\n",
        "        valid_loss /= len(val_loader)\n",
        "    else:\n",
        "        valid_loss = float('inf') # Or 0\n",
        "        print(f\"Warning: Validation loader is empty in epoch {epoch}.\")\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "\n",
        "    print(\"Epoch: {} Train Loss: {:.4f} Valid Loss: {:.4f}\".format(epoch + 1, train_loss, valid_loss))\n",
        "\n",
        "    # Save the model every 10 epochs\n",
        "    if ((epoch + 1) % 10 == 0):\n",
        "      # Corrected epoch number in the filename to match the loop iteration\n",
        "      model_path = get_model_name(model.name, batch_size, lr, epoch + 1)\n",
        "      # Ensure the directory exists before saving (redundant if os.makedirs was called earlier, but good practice)\n",
        "      # os.makedirs(os.path.dirname(model_path), exist_ok=True) # Handled by get_model_name + initial os.makedirs\n",
        "      torch.save(model.state_dict(), model_path)\n",
        "      print(f\"Saved model to {model_path}\")\n",
        "\n",
        "  print('Finished Training')\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "\n",
        "  # plotting\n",
        "  plt.figure() # Create a new figure for the plot\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters_loss, train_losses, label=\"Train Loss\") # Changed label to Loss\n",
        "  plt.plot(iters_loss, valid_losses, label=\"Validation Loss\") # Changed label to Loss\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  # Accuracy plotting requires a separate function for CTC decoding, which is not directly calculated here.\n",
        "  # The original accuracy calculation function `get_accuracy` is for classification, not sequence prediction with CTC.\n",
        "  # You would need to implement a CTC beam search or greedy decoding function to get accuracy.\n",
        "\n",
        "# Assuming the original `get_accuracy` is for the Baseline CNN and not applicable here.\n",
        "# Including a placeholder for accuracy plotting if you implement decoding later.\n",
        "# def plot_accuracy(epochs, train_accs, val_accs):\n",
        "#     plt.figure()\n",
        "#     plt.title(\"Accuracy Curve\")\n",
        "#     plt.plot(epochs, train_accs, label=\"Train Accuracy\")\n",
        "#     plt.plot(epochs, val_accs, label=\"Validation Accuracy\")\n",
        "#     plt.xlabel(\"Epochs\")\n",
        "#     plt.ylabel(\"Accuracy\")\n",
        "#     plt.legend(loc='best')\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "def decode_pred(pred):\n",
        "    # Greedy decoding for simplicity\n",
        "    # pred shape is (sequence_length, batch_size, num_classes+1)\n",
        "    # Take the argmax along the class dimension\n",
        "    _, max_preds = torch.max(pred, dim=2) # Shape: (sequence_length, batch_size)\n",
        "\n",
        "    decoded_preds = []\n",
        "    for batch_idx in range(max_preds.size(1)):\n",
        "        # Get the sequence for this item in the batch\n",
        "        sequence = max_preds[:, batch_idx].tolist()\n",
        "\n",
        "        decoded_sequence = []\n",
        "        last_char = None\n",
        "        for char_code in sequence:\n",
        "            # Apply CTC decoding rules: remove consecutive duplicates and blanks (code 0)\n",
        "            if char_code != last_char and char_code != 0:\n",
        "                # Map the character code back to the original character index (subtract 1)\n",
        "                # Then use that index to get the character from all_characters\n",
        "                if 0 < char_code <= len(all_characters): # Check bounds\n",
        "                    decoded_sequence.append(all_characters[char_code - 1])\n",
        "                else:\n",
        "                    # Handle unexpected character codes if necessary\n",
        "                    pass\n",
        "            last_char = char_code\n",
        "        decoded_preds.append(\"\".join(decoded_sequence))\n",
        "    return decoded_preds\n",
        "\n",
        "def decode_label(label_tensor):\n",
        "  # label_tensor is expected to be a 1D tensor of shape (label_length,)\n",
        "  decoded_label = []\n",
        "  for char_code in label_tensor.tolist():\n",
        "      # Map the character code back to the original character index (subtract 1)\n",
        "      # Then use that index to get the character from all_characters\n",
        "      if 0 < char_code <= len(all_characters): # Check bounds and skip blank (0)\n",
        "          decoded_label.append(all_characters[char_code - 1])\n",
        "      else:\n",
        "          # Handle unexpected character codes if necessary (shouldn't happen with valid labels)\n",
        "          pass\n",
        "\n",
        "  return \"\".join(decoded_label)\n",
        "\n",
        "\n",
        "# Updated generate_graph_data to load from the correct path and use CTCLoss for accuracy\n",
        "def generate_graph_data():\n",
        "  epochs = []\n",
        "  # We'll collect losses here as accuracy is complex with CTC\n",
        "  train_losses_hist, val_losses_hist = [], []\n",
        "\n",
        "  # Add initial point (epoch 0) if needed, but losses/acc will be 0/high\n",
        "  # epochs.append(0)\n",
        "  # train_losses_hist.append(0.0) # Or a placeholder\n",
        "  # val_losses_hist.append(0.0) # Or a placeholder\n",
        "\n",
        "  # Define the same batch size used for training\n",
        "  batch_size_for_eval = 128 # Or whatever batch size you used for train_CLSTM\n",
        "\n",
        "  # Get loaders for evaluation. No need for single loaders here.\n",
        "  # Using batch_size_for_eval for more efficient loading\n",
        "  train_loader_eval, val_loader_eval, test_loader_eval = get_data_loader(batch_size_for_eval)\n",
        "\n",
        "  # Check GPU availability and move model\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # Initialize a model instance outside the loop\n",
        "  model = CLSTM()\n",
        "  model.to(device)\n",
        "  criterion = nn.CTCLoss(blank=0)\n",
        "\n",
        "  print(\"Generating graph data from saved models...\")\n",
        "\n",
        "  # Iterate through the epochs where models were saved (every 10 epochs)\n",
        "  for i in range(10, 401, 10): # Go up to and including epoch 400\n",
        "    # Construct the model path using the updated get_model_name\n",
        "    model_path = get_model_name(model.name, 128, 1e-4, i) # Use the same parameters as training\n",
        "\n",
        "    # Check if the model file exists before trying to load it\n",
        "    if not os.path.exists(model_path):\n",
        "      print(f\"Warning: Model file not found for epoch {i} at {model_path}. Skipping.\")\n",
        "      continue # Skip this epoch if the file doesn't exist\n",
        "\n",
        "    try:\n",
        "        # Load the model state dictionary\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.eval() # Set model to evaluation mode\n",
        "\n",
        "        epochs.append(i)\n",
        "\n",
        "        # Calculate average training loss for this epoch\n",
        "        train_loss_epoch = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, labels, label_lengths) in enumerate(train_loader_eval):\n",
        "                if images is None: continue\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                label_lengths = label_lengths.to(device)\n",
        "                if torch.any(label_lengths <= 0) or torch.any(label_lengths > torch.full(size=(images.size(0),), fill_value=images.size(3) // (2*2), dtype=torch.int32).to(device)): continue\n",
        "                preds = model(images)\n",
        "                log_probs = F.log_softmax(preds, 2)\n",
        "                pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device)\n",
        "                loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "                train_loss_epoch += loss.item()\n",
        "            if len(train_loader_eval) > 0:\n",
        "                train_loss_epoch /= len(train_loader_eval)\n",
        "            else:\n",
        "                train_loss_epoch = float('inf')\n",
        "        train_losses_hist.append(train_loss_epoch)\n",
        "\n",
        "\n",
        "        # Calculate average validation loss for this epoch\n",
        "        val_loss_epoch = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, labels, label_lengths) in enumerate(val_loader_eval):\n",
        "                if images is None: continue\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                label_lengths = label_lengths.to(device)\n",
        "                if torch.any(label_lengths <= 0) or torch.any(label_lengths > torch.full(size=(images.size(0),), fill_value=images.size(3) // (2*2), dtype=torch.int32).to(device)): continue\n",
        "                preds = model(images)\n",
        "                log_probs = F.log_softmax(preds, 2)\n",
        "                pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device)\n",
        "                loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "                val_loss_epoch += loss.item()\n",
        "            if len(val_loader_eval) > 0:\n",
        "                val_loss_epoch /= len(val_loader_eval)\n",
        "            else:\n",
        "                val_loss_epoch = float('inf')\n",
        "        val_losses_hist.append(val_loss_epoch)\n",
        "\n",
        "        # Note: Calculating accuracy for CTC requires decoding, which is not handled by get_accuracy.\n",
        "        # If you implement a decoding function, you would calculate accuracy here.\n",
        "        # For now, the graph will show loss.\n",
        "\n",
        "        print(f\"Processed epoch {i}: Train Loss = {train_loss_epoch:.4f}, Validation Loss = {val_loss_epoch:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or evaluating model for epoch {i} from {model_path}: {e}\")\n",
        "        # Optionally, log more details or skip this epoch's data points\n",
        "        continue\n",
        "\n",
        "\n",
        "  # Plotting the loss curve\n",
        "  plt.figure() # Create a new figure for the plot\n",
        "  plt.title(\"Training and Validation Loss Curve\")\n",
        "  plt.plot(epochs, train_losses_hist, label=\"Train Loss\")\n",
        "  plt.plot(epochs, val_losses_hist, label=\"Validation Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  # If you calculate accuracy later, you can add another plotting section here.\n",
        "  # print(\"\\nEpochs recorded: {}\".format(epochs))\n",
        "  # print(\"Train Losses recorded: {}\".format(train_losses_hist))\n",
        "  # print(\"Validation Losses recorded: {}\".format(val_losses_hist))\n",
        "\n",
        "# Example usage for training the CLSTM (assuming data loaders are ready)\n",
        "# drive_path_processed_data = '/content/drive/MyDrive/Miniproj/DS/Processed'\n",
        "# train_loader, val_loader, test_loader = get_data_loader(128) # Use the same batch size as in train_CLSTM\n",
        "# model = CLSTM() # Initialize the model\n",
        "# train_CLSTM(model, train_loader, val_loader, batch_size=128, num_epochs=400, lr=1e-4)\n",
        "\n",
        "\n",
        "# Call generate_graph_data after training and saving models\n",
        "generate_graph_data()\n",
        "\n",
        "# Example usage for decoding a single image after training (requires loading the model)\n",
        "# This part was in a commented out cell, adding it here for completeness if you want to test inference\n",
        "# drive_path_processed_data = '/content/drive/MyDrive/Miniproj/DS/Processed'\n",
        "# train_loader_single, val_loader_single, test_loader_single = get_data_loader(1) # Get a loader with batch size 1\n",
        "\n",
        "# # Load the best performing model (e.g., epoch 400 based on your training)\n",
        "# model = CLSTM()\n",
        "# model_filename_final = get_model_name(model.name, 128, 1e-4, 400) # Use the final epoch number\n",
        "# model_path_final = os.path.join(model_save_dir, model_filename_final)\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#     model.load_state_dict(torch.load(model_path_final))\n",
        "#     model.cuda()\n",
        "# else:\n",
        "#     model.load_state_dict(torch.load(model_path_final, map_location='cpu'))\n",
        "\n",
        "# model.eval()\n",
        "\n",
        "# print(\"\\nDemonstrating inference on a test image:\")\n",
        "# with torch.no_grad():\n",
        "#     # Get one image from the test loader\n",
        "#     for images, labels, label_lengths in test_loader_single:\n",
        "#         if images is None: continue # Skip empty batch\n",
        "#         # Move image and labels to the same device as the model\n",
        "#         device = next(model.parameters()).device # Get model's device\n",
        "#         images = images.to(device)\n",
        "#         labels = labels.to(device) # Move labels to device for decode_label if needed\n",
        "#         label_lengths = label_lengths.to(device) # Move label_lengths to device\n",
        "\n",
        "#         # Forward pass\n",
        "#         preds = model(images) # Shape: (sequence_length, batch_size=1, num_classes+1)\n",
        "\n",
        "#         # Decode the prediction\n",
        "#         dec_pred = decode_pred(preds)\n",
        "\n",
        "#         # Decode the ground truth label\n",
        "#         # Since labels is a concatenated tensor, you need to extract the single label tensor for this batch\n",
        "#         # Assuming batch size is 1, labels is a 1D tensor of shape (label_length,)\n",
        "#         dec_label = decode_label(labels)\n",
        "\n",
        "#         print(f\"Predicted: {dec_pred[0]}\") # dec_pred is a list of strings\n",
        "#         print(f\"Ground Truth: {dec_label}\")\n",
        "\n",
        "#         # Optional: Display the image\n",
        "#         # You need a function to display the tensor image\n",
        "#         # Assuming show_image is defined elsewhere and can handle the tensor format\n",
        "#         # show_image(images.cpu().squeeze(0)) # Move back to CPU for plotting\n",
        "\n",
        "#         break # Process only the first image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSUqfIFRG8MX"
      },
      "outputs": [],
      "source": [
        "train_accs = [0.0, 0.0, 0.009859872611464968, 0.010789808917197453, 0.013363057324840765, 0.015031847133757962, 0.03211464968152866, 0.05419108280254777, 0.08047133757961783, 0.09649681528662421, 0.09005095541401274, 0.14239490445859873, 0.1699235668789809, 0.20080254777070064, 0.23828025477707007, 0.27845859872611467, 0.31142675159235667, 0.3516050955414013, 0.39038216560509553, 0.4108152866242038, 0.4487388535031847]\n",
        "val_accs = [0.0, 0.0, 0.010372931588046431, 0.008446530007409237, 0.010323536675722401, 0.014028155100024697, 0.027315386515188937, 0.04687577179550506, 0.0704865398863917, 0.08446530007409236, 0.07942701901704124, 0.1259570264262781, 0.1503581131143492, 0.172585823660163, 0.20617436404050382, 0.23378612002963695, 0.2609533218078538, 0.29928377377130155, 0.3268955297604347, 0.3475919980242035, 0.37831563348975056]\n",
        "\n",
        "# model = model.cuda()\n",
        "# for images, labels in test_loader_single:\n",
        "#   images = images.cuda()\n",
        "#   pred = model(images)\n",
        "#   dec_pred = decode_pred(pred)\n",
        "#   dec_label = decode_labels(labels)\n",
        "\n",
        "#   print(\"Pred: {}\".format(dec_pred))\n",
        "#   print(\"Label: {}\".format(dec_label))\n",
        "\n",
        "def decode_label(label):\n",
        "  decoded_label = []\n",
        "  for char in label:\n",
        "    decoded_label.append(all_characters[int(char)])\n",
        "\n",
        "  return decoded_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTo_w2fmjoaE"
      },
      "source": [
        "### **Data Demonstration:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7DLpzVdfrvV",
        "outputId": "ab4b6338-ffcb-46a8-8df2-f400bfa1564c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# from torchvision.io import read_image # Not used\n",
        "from torch.utils import data\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow # Commented out as it's not used and might cause issues\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Ensure Google Drive is mounted if it wasn't already\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Ensure all_characters is defined as it's used in decode_label and decode_pred\n",
        "all_characters = string.ascii_letters + string.digits\n",
        "\n",
        "# Define the directory for saving models within your Google Drive\n",
        "# Adjust this path if you prefer a different location\n",
        "model_save_dir = '/content/gdrive/My Drive/Miniproj/Models'\n",
        "\n",
        "# Create the model save directory if it doesn't exist\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "  \"\"\" Generate a name for the model including the save directory \"\"\"\n",
        "  # Construct the filename\n",
        "  filename = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,batch_size,learning_rate,epoch)\n",
        "  # Combine the save directory path with the filename\n",
        "  # Ensure the directory exists before returning the path (can be done here or before saving)\n",
        "  # os.makedirs(model_save_dir, exist_ok=True) # Already done outside this function\n",
        "  path = os.path.join(model_save_dir, filename)\n",
        "  return path\n",
        "\n",
        "# Assuming CLSTM class is defined in a previous cell and is correct.\n",
        "# Including the definition here for completeness based on ipython-input-132\n",
        "class CLSTM(nn.Module):\n",
        "  def __init__(self, input_size=256, hidden_size=128, num_class=62):\n",
        "    super(CLSTM, self).__init__()\n",
        "    self.name = \"CLSTM\"\n",
        "    self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "\n",
        "    # Dynamically calculate the input size for the first fully connected layer (self.fc1)\n",
        "    # This is done by passing a dummy tensor through the convolutional and pooling layers\n",
        "    # Assume input image size is 70x250 (from the get_data_loader transform)\n",
        "    dummy_input = torch.randn(1, 1, 70, 250)\n",
        "    x = F.relu(self.conv1(dummy_input))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.view(x.size(0), x.size(1), -1)\n",
        "    input_feature_size = x.shape[2]\n",
        "\n",
        "    self.fc1 = nn.Linear(input_feature_size, input_size)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=True, num_layers=2, batch_first=True)\n",
        "    self.fc2 = nn.Linear(hidden_size*2, num_class+1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    # Bring the width to the middle\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.view(batch_size, x.size(1), -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x, _ = self.rnn(x)\n",
        "    x = self.fc2(x)\n",
        "    # Put in required shape for to get CTC loss\n",
        "    x = x.permute(1, 0, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "# Assuming string_label function is defined in a previous cell and is correct.\n",
        "# Including placeholder definition\n",
        "def string_label(string):\n",
        "  out = []\n",
        "  # Using a loop to map characters to indices based on all_characters\n",
        "  char_to_index = {char: i for i, char in enumerate(all_characters)}\n",
        "  for char_string in string:\n",
        "      if char_string in char_to_index:\n",
        "          # CTC blank is often 0. Labels should start from 1.\n",
        "          # So, map char index (0 to 61) to label (1 to 62).\n",
        "          out.append(char_to_index[char_string] + 1)\n",
        "      else:\n",
        "          # Handle characters not in all_characters if necessary\n",
        "          # print(f\"Warning: Character '{char_string}' not found in all_characters.\")\n",
        "          pass # Or append a specific value to indicate unknown character if needed\n",
        "\n",
        "  return out\n",
        "\n",
        "\n",
        "class CAPTCHAImageDataset(data.Dataset):\n",
        "  def __init__(self, img_dir, transform=None):\n",
        "    if not os.path.exists(img_dir):\n",
        "        # Corrected the error message to show the directory path\n",
        "        raise FileNotFoundError(f\"Image directory not found: {img_dir}\")\n",
        "    # Filter out directories and non-image files if necessary, though os.listdir is usually fine\n",
        "    self.all_images = [f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.all_images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if idx >= len(self.all_images):\n",
        "         raise IndexError(f\"Index {idx} out of bounds for dataset of size {len(self)}\")\n",
        "\n",
        "    img_path_relative = self.all_images[idx]\n",
        "    full_img_path = os.path.join(self.img_dir, img_path_relative)\n",
        "\n",
        "    # Extract label from filename\n",
        "    label_filename = os.path.splitext(img_path_relative)[0] # Get filename without extension\n",
        "    # Assuming filename format is 'Label-OptionalStuff.jpg' or just 'Label.jpg'\n",
        "    # Take the part before the first hyphen or the whole name if no hyphen\n",
        "    parts = label_filename.split(\"-\")\n",
        "    label = parts[0]\n",
        "\n",
        "    # Corrected image loading to handle potential issues and ensure grayscale for single channel model\n",
        "    try:\n",
        "        # Convert to grayscale ('L') to match the model's single input channel\n",
        "        image = Image.open(full_img_path).convert(\"L\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error opening image file: {full_img_path}\")\n",
        "        # Return None or handle the error appropriately\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing image {full_img_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "\n",
        "    # Ensure labels are processed correctly for CTC Loss\n",
        "    labels_list = string_label(label) # This now returns indices + 1\n",
        "    if not labels_list:\n",
        "        # Handle cases where string_label returns an empty list (e.g., label has no valid characters)\n",
        "        # Or if the extracted label string was empty\n",
        "        print(f\"Warning: Could not generate labels for filename {img_path_relative} (label: '{label}'). Skipping.\")\n",
        "        return None, None\n",
        "\n",
        "    # CTC Loss typically expects a sequence of integers. string_label already returns a list of ints.\n",
        "    # Convert the list to a LongTensor\n",
        "    labels = torch.LongTensor(labels_list)\n",
        "\n",
        "\n",
        "    return image, labels\n",
        "\n",
        "# Updated get_data_loader function to use the custom collate function\n",
        "def get_data_loader(batch_size, is_split_data=False):\n",
        "  # Use the correct base path for the processed data\n",
        "  drive_path_dataset = '/content/gdrive/My Drive/Miniproj/DS/Processed'\n",
        "\n",
        "  transform = transforms.Compose([transforms.Resize((70,250)),transforms.ToTensor()])\n",
        "\n",
        "  # Note: is_split_data=True is intended for the Baseline CNN and uses different dimensions/data structure.\n",
        "  # This CLSTM model expects the full 70x250 image.\n",
        "  if is_split_data:\n",
        "      print(\"Warning: get_data_loader called with is_split_data=True for CLSTM model. This might not be intended.\")\n",
        "      # Keeping the original transform for the full image for the CLSTM\n",
        "      pass\n",
        "\n",
        "\n",
        "  # Corrected paths to point to the subdirectories within the processed data folder\n",
        "  train_data = CAPTCHAImageDataset(os.path.join(drive_path_dataset, \"Saved_Train\"), transform=transform)\n",
        "  val_data = CAPTCHAImageDataset(os.path.join(drive_path_dataset, \"Saved_Validation\"), transform=transform)\n",
        "  test_data = CAPTCHAImageDataset(os.path.join(drive_path_dataset, \"Saved_Test\"), transform=transform)\n",
        "\n",
        "  # Custom collate function to handle variable sequence lengths for CTC loss\n",
        "  def collate_fn(batch):\n",
        "      # Filter out samples where loading/processing failed (returned None, None)\n",
        "      batch = [item for item in batch if item is not None and item[0] is not None and item[1] is not None]\n",
        "      if not batch:\n",
        "          return None, None, None # Return None for all outputs if the batch is empty after filtering\n",
        "\n",
        "      # Separate images and labels\n",
        "      images, labels = zip(*batch)\n",
        "\n",
        "      # Stack images (they should already be tensors from the transform)\n",
        "      images = torch.stack(images, 0)\n",
        "\n",
        "      # Labels need to be concatenated and provided with lengths for CTCLoss\n",
        "      # labels is a tuple of LongTensors from string_label\n",
        "      label_lengths = torch.LongTensor([len(label) for label in labels])\n",
        "      labels = torch.cat(labels, 0) # Concatenate all individual label tensors into one long tensor\n",
        "\n",
        "      return images, labels, label_lengths\n",
        "\n",
        "  # Use the custom collate function\n",
        "  train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn, pin_memory=True if torch.cuda.is_available() else False)\n",
        "  val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn, pin_memory=True if torch.cuda.is_available() else False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=2, shuffle=True, collate_fn=collate_fn, pin_memory=True if torch.cuda.is_available() else False)\n",
        "\n",
        "\n",
        "  return train_loader, val_loader, test_loader\n",
        "\n",
        "# Updated train_CLSTM function\n",
        "def train_CLSTM(model, train_loader, val_loader, batch_size=128, num_epochs=400, lr=1e-4):\n",
        "\n",
        "  # Check GPU availability and move model\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "  print(f\"Training on: {device}\")\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  # CTCLoss expects blank=0. Our labels are 1-62. So blank is 0.\n",
        "  criterion = nn.CTCLoss(blank=0, reduction='mean') # Use mean reduction for average loss per batch\n",
        "\n",
        "  iters_loss, train_losses, valid_losses = [], [], []\n",
        "\n",
        "  print(\"Starting CLSTM Training...\")\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    train_loss = 0\n",
        "    # Updated data loading to match collate_fn output: images, labels, label_lengths\n",
        "    # Use enumerate to get batch_idx if needed for logging\n",
        "    for batch_idx, batch_data in enumerate(train_loader):\n",
        "      images, labels, label_lengths = batch_data # Unpack the tuple from collate_fn\n",
        "\n",
        "      if images is None:\n",
        "          # Skip if the batch was empty after filtering in collate_fn\n",
        "          print(f\"Warning: Empty batch at epoch {epoch}, batch {batch_idx}. Skipping.\")\n",
        "          continue\n",
        "\n",
        "      images = images.to(device)\n",
        "      # Labels and label_lengths are needed by CTCLoss on the same device\n",
        "      labels = labels.to(device)\n",
        "      label_lengths = label_lengths.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      preds = model(images) # Shape: (sequence_length, batch_size, num_classes+1)\n",
        "\n",
        "      log_probs = F.log_softmax(preds, 2) # Shape: (sequence_length, batch_size, num_classes+1)\n",
        "      # pred_lengths should be the length of the sequence dimension for each item in the batch\n",
        "      # This is the first dimension of log_probs, repeated batch_size times\n",
        "      pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device) # Ensure on device\n",
        "\n",
        "      # Check for potential issues with label_lengths or pred_lengths\n",
        "      # CTCLoss requires label_lengths <= pred_lengths\n",
        "      if torch.any(label_lengths <= 0):\n",
        "           # This should ideally be handled in string_label or dataset __getitem__\n",
        "           print(f\"Warning: Zero or negative label length detected in epoch {epoch}, batch {batch_idx}. Skipping batch.\")\n",
        "           continue\n",
        "      if torch.any(label_lengths > pred_lengths):\n",
        "           # This can happen if image processing/transforms lead to very short sequences\n",
        "           # or if labels are unexpectedly long. The CLSTM architecture output sequence length\n",
        "           # depends on the convolutional/pooling layers and image width.\n",
        "           # For 70x250 input, Conv(3)+Pool(2)+Conv(3)+Pool(2) results in output width:\n",
        "           # (250 - 3 + 1)/1 = 248 -> 248/2 = 124 -> (124 - 3 + 1)/1 = 122 -> 122/2 = 61\n",
        "           # The sequence length is 61. If any label is > 61, CTCLoss will error.\n",
        "           print(f\"Warning: Label length exceeds sequence length in epoch {epoch}, batch {batch_idx}. Max label length is {label_lengths.max().item()}, sequence length is {pred_lengths.max().item()}. Skipping batch.\")\n",
        "           continue\n",
        "\n",
        "\n",
        "      try:\n",
        "          # CTCLoss(log_probs, targets, input_lengths, target_lengths)\n",
        "          loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "          loss.backward()\n",
        "          # Gradient clipping to prevent exploding gradients, common in RNNs/LSTMs\n",
        "          # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "          optimizer.step()\n",
        "          train_loss += loss.item() # Accumulate the mean loss for the batch\n",
        "      except Exception as e:\n",
        "          print(f\"Error during loss calculation/backward pass in epoch {epoch}, batch {batch_idx}: {e}\")\n",
        "          # Optionally, log more details or skip the batch\n",
        "          continue # Skip to the next batch if an error occurs\n",
        "\n",
        "    iters_loss.append(epoch)\n",
        "    # Ensure train_loss is not divided by zero if train_loader is empty or all batches skipped\n",
        "    if len(train_loader) > 0 and (batch_idx + 1) > 0: # Check if at least one batch was processed\n",
        "        # Calculate average loss over processed batches\n",
        "        # train_loss has accumulated the mean loss per batch, so divide by number of processed batches\n",
        "        avg_train_loss = train_loss / (batch_idx + 1) # Use batch_idx + 1 as counter\n",
        "    else:\n",
        "        avg_train_loss = float('inf') # Or 0, depending on desired behavior for empty loader\n",
        "        if len(train_loader) == 0:\n",
        "            print(f\"Warning: Train loader is empty in epoch {epoch}. No training occurred.\")\n",
        "        else:\n",
        "             print(f\"Warning: All batches skipped in train loader in epoch {epoch}. No training occurred.\")\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    valid_loss = 0\n",
        "    num_valid_batches = 0\n",
        "    with torch.no_grad(): # Disable gradient calculation for validation\n",
        "        # Updated data loading to match collate_fn output\n",
        "        for batch_idx, batch_data in enumerate(val_loader):\n",
        "          images, labels, label_lengths = batch_data\n",
        "\n",
        "          if images is None:\n",
        "              print(f\"Warning: Empty validation batch at epoch {epoch}, batch {batch_idx}. Skipping.\")\n",
        "              continue\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          label_lengths = label_lengths.to(device)\n",
        "\n",
        "          if torch.any(label_lengths <= 0):\n",
        "              print(f\"Warning: Invalid label length in validation epoch {epoch}, batch {batch_idx}. Skipping batch.\")\n",
        "              continue\n",
        "          # Re-calculate sequence length for safety, though it should be consistent\n",
        "          sequence_length = images.size(3) // (2 * 2) # Assuming 2 conv+pool steps with kernel/stride 2\n",
        "          if torch.any(label_lengths > torch.full(size=(images.size(0),), fill_value=sequence_length, dtype=torch.int32).to(device)):\n",
        "               print(f\"Warning: Label length exceeds sequence length in validation epoch {epoch}, batch {batch_idx}. Skipping batch.\")\n",
        "               continue\n",
        "\n",
        "\n",
        "          try:\n",
        "              preds = model(images)\n",
        "              log_probs = F.log_softmax(preds, 2)\n",
        "              pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device)\n",
        "              loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "              valid_loss += loss.item() # Accumulate the mean loss for the batch\n",
        "              num_valid_batches += 1\n",
        "          except Exception as e:\n",
        "              print(f\"Error during validation loss calculation in epoch {epoch}, batch {batch_idx}: {e}\")\n",
        "              continue # Skip to the next batch\n",
        "\n",
        "\n",
        "    # Ensure valid_loss is not divided by zero if val_loader is empty or all batches skipped\n",
        "    if num_valid_batches > 0:\n",
        "        avg_valid_loss = valid_loss / num_valid_batches # Average over processed batches\n",
        "    else:\n",
        "        avg_valid_loss = float('inf') # Or 0\n",
        "        if len(val_loader) == 0:\n",
        "            print(f\"Warning: Validation loader is empty in epoch {epoch}. No validation occurred.\")\n",
        "        else:\n",
        "            print(f\"Warning: All batches skipped in validation loader in epoch {epoch}. No validation occurred.\")\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "\n",
        "\n",
        "    print(\"Epoch: {} Train Loss: {:.4f} Valid Loss: {:.4f}\".format(epoch + 1, avg_train_loss, avg_valid_loss))\n",
        "\n",
        "    # Save the model every 10 epochs\n",
        "    if ((epoch + 1) % 10 == 0):\n",
        "      # Corrected epoch number in the filename to match the loop iteration\n",
        "      model_path = get_model_name(model.name, batch_size, lr, epoch + 1) # Use actual batch_size and lr\n",
        "      torch.save(model.state_dict(), model_path)\n",
        "      print(f\"Saved model to {model_path}\")\n",
        "\n",
        "  print('Finished Training')\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "\n",
        "  # plotting\n",
        "  plt.figure() # Create a new figure for the plot\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(iters_loss, train_losses, label=\"Train Loss\") # Changed label to Loss\n",
        "  plt.plot(iters_loss, valid_losses, label=\"Validation Loss\") # Changed label to Loss\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  # Accuracy plotting requires a separate function for CTC decoding, which is not directly calculated here.\n",
        "  # The original accuracy calculation function `get_accuracy` is for classification, not sequence prediction with CTC.\n",
        "  # You would need to implement a CTC beam search or greedy decoding function to get accuracy.\n",
        "\n",
        "\n",
        "def decode_pred(pred_tensor, all_characters, blank_idx=0):\n",
        "    # pred_tensor is model output, shape (sequence_length, batch_size, num_classes+1)\n",
        "    # For batch_size=1, shape is (sequence_length, 1, num_classes+1)\n",
        "    if pred_tensor.ndim == 3 and pred_tensor.shape[1] == 1:\n",
        "        # Squeeze the batch dimension if it's 1\n",
        "        pred_tensor = pred_tensor.squeeze(1) # Shape becomes (sequence_length, num_classes+1)\n",
        "    elif pred_tensor.ndim != 2:\n",
        "         # This case might occur if batch size > 1 is passed\n",
        "         # For decoding, usually process batch items one by one or handle batched decoding.\n",
        "         # This function is currently designed for a single prediction sequence.\n",
        "         raise ValueError(f\"decode_pred expects tensor shape (seq_len, num_classes+1) or (seq_len, 1, num_classes+1), but got {pred_tensor.shape}\")\n",
        "\n",
        "\n",
        "    # Use argmax to get the predicted class index for each time step\n",
        "    argmax_preds = torch.argmax(pred_tensor, dim=1).cpu().numpy()\n",
        "\n",
        "    decoded_chars = []\n",
        "    # Simple greedy decoding: remove consecutive duplicates and blanks\n",
        "    last_decoded_char = None # Store the last character added to the decoded string\n",
        "\n",
        "    for idx in argmax_preds:\n",
        "        if idx != blank_idx: # If the current prediction is not the blank symbol (0)\n",
        "            # Adjust index for all_characters if blank_idx is 0\n",
        "            # Our string_label adds 1, so indices in pred are 1-62 for chars, 0 for blank.\n",
        "            # To get the original index in all_characters (0-61), subtract 1.\n",
        "            char_index = idx - 1\n",
        "\n",
        "            if 0 <= char_index < len(all_characters):\n",
        "                 current_char = all_characters[char_index]\n",
        "                 # Append the character if it's the first character OR\n",
        "                 # it's different from the last non-blank character added\n",
        "                 if not decoded_chars or current_char != last_decoded_char:\n",
        "                      decoded_chars.append(current_char)\n",
        "                      last_decoded_char = current_char # Update the last added character\n",
        "            else:\n",
        "                 # This case indicates an issue with model output if indices are out of expected range\n",
        "                 print(f\"Warning: decode_pred encountered out-of-bounds character index: {char_index} (from predicted index {idx})\")\n",
        "                 pass # Ignore invalid index\n",
        "\n",
        "        else: # If the current prediction IS the blank symbol (0)\n",
        "            # Skip blank symbols in the output\n",
        "            pass\n",
        "\n",
        "\n",
        "    return \"\".join(decoded_chars) # Join characters into a string\n",
        "\n",
        "def decode_label(label_tensor, all_characters):\n",
        "  # label_tensor is expected to be a 1D LongTensor of shape (label_length,)\n",
        "  # Labels were encoded with +1 (1 to 62). Blank is 0 but should not appear in true labels.\n",
        "  decoded_label = []\n",
        "  for char_code in label_tensor.tolist(): # Convert tensor to list of integers\n",
        "      # Map the character code back to the original character index (subtract 1)\n",
        "      char_index_adjusted = int(char_code) - 1 # Subtract 1 to get 0-61 index\n",
        "      # Then use that index to get the character from all_characters\n",
        "      if 0 <= char_index_adjusted < len(all_characters): # Check bounds\n",
        "          decoded_label.append(all_characters[char_index_adjusted])\n",
        "      else:\n",
        "          # This case shouldn't happen for true labels if encoding is correct\n",
        "          print(f\"Warning: decode_label encountered out-of-bounds character index: {char_index_adjusted} (from label code {char_code})\")\n",
        "          decoded_label.append(\"?\") # Use a placeholder for unknown index\n",
        "\n",
        "  return \"\".join(decoded_label)\n",
        "\n",
        "\n",
        "# Updated generate_graph_data to load from the correct path and plot loss\n",
        "def generate_graph_data():\n",
        "  epochs = []\n",
        "  # We'll collect losses here as accuracy calculation with CTC requires decoding\n",
        "  train_losses_hist, val_losses_hist = [], []\n",
        "\n",
        "  # Define the same batch size used for training for consistent evaluation\n",
        "  batch_size_for_eval = 128 # Or whatever batch size you used for train_CLSTM\n",
        "\n",
        "  # Get loaders for evaluation. Use the batch size for eval.\n",
        "  train_loader_eval, val_loader_eval, test_loader_eval = get_data_loader(batch_size_for_eval)\n",
        "\n",
        "  # Check GPU availability and move model\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # Initialize a model instance outside the loop\n",
        "  model = CLSTM()\n",
        "  model.to(device)\n",
        "  criterion = nn.CTCLoss(blank=0, reduction='mean') # Use mean reduction for consistency\n",
        "\n",
        "  print(\"Generating graph data from saved models...\")\n",
        "\n",
        "  # Iterate through the epochs where models were saved (every 10 epochs)\n",
        "  # Adjust the range based on your train_CLSTM save logic (e.g., range(10, num_epochs + 1, 10))\n",
        "  num_epochs_trained = 400 # Assuming 400 epochs were trained and saved every 10\n",
        "  for i in range(10, num_epochs_trained + 1, 10): # Go up to and including the last epoch saved\n",
        "    # Construct the model path using the updated get_model_name\n",
        "    model_path = get_model_name(model.name, 128, 1e-4, i) # Use the same parameters as training\n",
        "\n",
        "    # Check if the model file exists before trying to load it\n",
        "    if not os.path.exists(model_path):\n",
        "      print(f\"Warning: Model file not found for epoch {i} at {model_path}. Skipping.\")\n",
        "      continue # Skip this epoch if the file doesn't exist\n",
        "    elif os.path.isdir(model_path):\n",
        "         print(f\"Warning: Path {model_path} is a directory, not a file. Skipping epoch {i}.\")\n",
        "         continue # Skip if it's a directory\n",
        "\n",
        "    try:\n",
        "        # Load the model state dictionary\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.eval() # Set model to evaluation mode\n",
        "\n",
        "        epochs.append(i)\n",
        "\n",
        "        # Calculate average training loss for this epoch\n",
        "        train_loss_epoch = 0\n",
        "        num_train_batches_processed = 0\n",
        "        with torch.no_grad():\n",
        "            # Use enumerate to get batch_idx if needed for logging\n",
        "            for batch_idx, batch_data in enumerate(train_loader_eval):\n",
        "                images, labels, label_lengths = batch_data\n",
        "                if images is None: continue\n",
        "\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                label_lengths = label_lengths.to(device)\n",
        "\n",
        "                # Check label lengths vs sequence lengths before calculating loss\n",
        "                sequence_length = images.size(3) // (2 * 2) # Assuming 2 conv+pool steps with kernel/stride 2\n",
        "                if torch.any(label_lengths <= 0) or torch.any(label_lengths > torch.full(size=(images.size(0),), fill_value=sequence_length, dtype=torch.int32).to(device)):\n",
        "                     # print(f\"Warning: Invalid label/sequence length in train eval epoch {i}, batch {batch_idx}. Skipping batch.\")\n",
        "                     continue\n",
        "\n",
        "                preds = model(images)\n",
        "                log_probs = F.log_softmax(preds, 2)\n",
        "                pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device)\n",
        "\n",
        "                try:\n",
        "                    loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "                    train_loss_epoch += loss.item()\n",
        "                    num_train_batches_processed += 1\n",
        "                except Exception as e:\n",
        "                     print(f\"Error calculating train eval loss in epoch {i}, batch {batch_idx}: {e}\")\n",
        "                     continue # Skip this batch\n",
        "\n",
        "            if num_train_batches_processed > 0:\n",
        "                train_loss_epoch /= num_train_batches_processed\n",
        "            else:\n",
        "                train_loss_epoch = float('inf') # Handle case where all batches are skipped\n",
        "                if len(train_loader_eval) > 0:\n",
        "                     print(f\"Warning: All batches skipped in train eval loader in epoch {i}. No train loss calculated.\")\n",
        "                else:\n",
        "                     print(f\"Warning: Train eval loader is empty in epoch {i}. No train loss calculated.\")\n",
        "\n",
        "        train_losses_hist.append(train_loss_epoch)\n",
        "\n",
        "\n",
        "        # Calculate average validation loss for this epoch\n",
        "        val_loss_epoch = 0\n",
        "        num_valid_batches_processed = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch_data in enumerate(val_loader_eval):\n",
        "                images, labels, label_lengths = batch_data\n",
        "                if images is None: continue\n",
        "\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                label_lengths = label_lengths.to(device)\n",
        "\n",
        "                # Check label lengths vs sequence lengths before calculating loss\n",
        "                sequence_length = images.size(3) // (2 * 2)\n",
        "                if torch.any(label_lengths <= 0) or torch.any(label_lengths > torch.full(size=(images.size(0),), fill_value=sequence_length, dtype=torch.int32).to(device)):\n",
        "                     # print(f\"Warning: Invalid label/sequence length in val eval epoch {i}, batch {batch_idx}. Skipping batch.\")\n",
        "                     continue\n",
        "\n",
        "                preds = model(images)\n",
        "                log_probs = F.log_softmax(preds, 2)\n",
        "                pred_lengths = torch.full(size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.int32).to(device)\n",
        "\n",
        "                try:\n",
        "                    loss = criterion(log_probs, labels, pred_lengths, label_lengths)\n",
        "                    val_loss_epoch += loss.item()\n",
        "                    num_valid_batches_processed += 1\n",
        "                except Exception as e:\n",
        "                     print(f\"Error calculating val eval loss in epoch {i}, batch {batch_idx}: {e}\")\n",
        "                     continue # Skip this batch\n",
        "\n",
        "            if num_valid_batches_processed > 0:\n",
        "                val_loss_epoch /= num_valid_batches_processed\n",
        "            else:\n",
        "                val_loss_epoch = float('inf') # Handle case where all batches are skipped\n",
        "                if len(val_loader_eval) > 0:\n",
        "                     print(f\"Warning: All batches skipped in val eval loader in epoch {i}. No validation loss calculated.\")\n",
        "                else:\n",
        "                     print(f\"Warning: Validation eval loader is empty in epoch {i}. No validation loss calculated.\")\n",
        "\n",
        "        val_losses_hist.append(val_loss_epoch)\n",
        "\n",
        "        print(f\"Epoch {i}: Train Loss = {train_loss_epoch:.4f}, Validation Loss = {val_loss_epoch:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch potential errors during model loading or evaluation for a specific epoch\n",
        "        print(f\"Error processing epoch {i}: {e}\")\n",
        "\n",
        "\n",
        "  print(\"Finished generating graph data.\")\n",
        "\n",
        "  plt.figure()\n",
        "  plt.title(\"Loss Curve from Saved Models\")\n",
        "  plt.plot(epochs, train_losses_hist, label=\"Train Loss\")\n",
        "  plt.plot(epochs, val_losses_hist, label=\"Validation Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "# Example usage (Assuming train_CLSTM is called first to train and save models)\n",
        "# train_loader, val_loader, test_loader = get_data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}